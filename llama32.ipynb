{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/regllava/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 04:35:30,297] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Added to sys.path: /home/ubuntu/Projects/regllava\n",
      "✔ CustomVisionTransformer imported.\n",
      "▶ Loading LLaVA model...\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 16743.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Projects/regllava/INFERclipregXGATED 에서 INFERclipregXGATED 모듈을 성공적으로 임포트했습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n",
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.5 and are newly initialized: ['model.mm_projector.0.bias', 'model.mm_projector.0.weight', 'model.mm_projector.2.bias', 'model.mm_projector.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIPVisionTower 로드 시 사용할 target_dtype: torch.float32 (fp16: False, bf16: False)\n",
      "로딩 시작: 사용자 정의 Reg-Gated CLIP 모델 (./models/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors)\n",
      "Checking model parameters... You should see [261, _ ]. If you see [257, _ ], something is wrong.\n",
      "vision_width, vision_layers, patch_size, grid_size:, new_pos_embed: 1024, 24, 14, 16, 261\n",
      "사용자 정의 Reg‑Gated CLIP 모델 로드 완료. 입력 해상도: 224, 패치 크기: 14\n",
      "✅ Model loaded.\n",
      "Conversation mode → llava_v1\n",
      "Loaded image: (1300, 954)\n",
      "\n",
      "--- Prompt to tokenizer ---\n",
      "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <im_patch>\n",
      "Describe the car in the image and its surroundings in detail. ASSISTANT:\n",
      "\n",
      "[USER] Describe the car in the image and its surroundings in detail.\n",
      "[ASSISTANT] "
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Float and Half",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 193\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroles[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode():\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Inference Complete. ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/regllava/llava/model/language_model/llava_llama.py:159\u001b[39m, in \u001b[36mLlavaLlamaForCausalLM.generate\u001b[39m\u001b[34m(self, inputs, images, image_sizes, **kwargs)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m`inputs_embeds` is not supported\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    152\u001b[39m     (\n\u001b[32m    153\u001b[39m         inputs,\n\u001b[32m    154\u001b[39m         position_ids,\n\u001b[32m    155\u001b[39m         attention_mask,\n\u001b[32m    156\u001b[39m         _,\n\u001b[32m    157\u001b[39m         inputs_embeds,\n\u001b[32m    158\u001b[39m         _\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_labels_for_multimodal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_sizes\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    169\u001b[39m     inputs_embeds = \u001b[38;5;28mself\u001b[39m.get_model().embed_tokens(inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/regllava/llava/model/llava_arch.py:243\u001b[39m, in \u001b[36mLlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal\u001b[39m\u001b[34m(self, input_ids, position_ids, attention_mask, past_key_values, labels, images, image_sizes)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected mm_patch_merge_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.mm_patch_merge_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     image_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# TODO: image start / end is not implemented here to support pretraining.\u001b[39;00m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[33m'\u001b[39m\u001b[33mtune_mm_mlp_adapter\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[33m'\u001b[39m\u001b[33mmm_use_im_start_end\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/regllava/llava/model/llava_arch.py:183\u001b[39m, in \u001b[36mLlavaMetaForCausalLM.encode_images\u001b[39m\u001b[34m(self, images)\u001b[39m\n\u001b[32m    180\u001b[39m     feats = feats[:, :num_cls_patch]\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# ⑤ projector\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm_projector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 must have the same dtype, but got Float and Half"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LLaVA Custom Vision Encoder/Projector Inference Script\n",
    "# =============================================================================\n",
    "\n",
    "# 0. 환경 설정 & 공통 라이브러리\n",
    "import os, sys, warnings, shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-12.4\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "    \"/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.4/lib64:\"\n",
    "    + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    ")\n",
    "\n",
    "import math\n",
    "import cv2  # OpenCV\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests  # For loading image from URL\n",
    "from threading import Thread\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    ")\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_PATCH_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import process_images, tokenizer_image_token, get_model_name_from_path\n",
    "from llava.model.builder import load_pretrained_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# LLaVA 프로젝트 루트가 sys.path 에 없으면 추가\n",
    "nb_root = os.getcwd()\n",
    "if nb_root not in sys.path:\n",
    "    sys.path.insert(0, nb_root)\n",
    "    print(f\"Added to sys.path: {nb_root}\")\n",
    "\n",
    "# 사용자 정의 CLIP 모듈 임포트\n",
    "from INFERclipregXGATED.model import VisionTransformer as CustomVisionTransformer\n",
    "print(\"✔ CustomVisionTransformer imported.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 사용자 설정\n",
    "# =============================================================================\n",
    "MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR = \"./llava-v1.5-7b-local\"  # 실제 경로로 수정\n",
    "MODEL_BASE_LLM = \"lmsys/vicuna-7b-v1.5\"                        # 사용할 LLM\n",
    "\n",
    "CUSTOM_VISION_ENCODER_WEIGHTS_PATH = \"./models/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors\"\n",
    "CUSTOM_PROJECTOR_FILENAME = \"mm_projector.bin\"\n",
    "\n",
    "IMAGE_FILE_TO_PROCESS = \"data/car.jpg\"\n",
    "USER_PROMPT = \"Describe the car in the image and its surroundings in detail.\"\n",
    "\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.2\n",
    "CONV_MODE = None  # None 이면 자동 추론 (\"vicuna_v1\" 등 지정 가능)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOAD_8BIT = False\n",
    "LOAD_4BIT = False\n",
    "\n",
    "VISION_ENCODER_CONFIG = {\n",
    "    \"image_resolution\": 224,\n",
    "    \"patch_size\": 14,\n",
    "    \"width\": 1024,\n",
    "    \"layers\": 24,\n",
    "    \"heads\": 16,\n",
    "    \"output_dim\": 1024,\n",
    "    \"num_registers\": 4\n",
    "}\n",
    "mm_vision_select_layer_val = -2\n",
    "mm_projector_type_val = \"mlp2x_gelu\"\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 유틸리티 함수 정의\n",
    "# =============================================================================\n",
    "def load_image(path_or_url: str) -> Image.Image:\n",
    "    if path_or_url.startswith((\"http://\", \"https://\")):\n",
    "        resp = requests.get(path_or_url)\n",
    "        resp.raise_for_status()\n",
    "        return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    if not os.path.exists(path_or_url):\n",
    "        raise FileNotFoundError(f\"Image file not found: {path_or_url}\")\n",
    "    return Image.open(path_or_url).convert(\"RGB\")\n",
    "\n",
    "def overlay_heatmap(heatmap_np: np.ndarray, base_img: Image.Image) -> np.ndarray:\n",
    "    bgr = cv2.cvtColor(np.array(base_img), cv2.COLOR_RGB2BGR)\n",
    "    h, w = bgr.shape[:2]\n",
    "    hm = cv2.resize(heatmap_np.astype(np.float32), (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    hm = (hm - hm.min()) / (hm.ptp() + 1e-8)\n",
    "    hm_cm = cv2.applyColorMap((hm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    mix = cv2.addWeighted(bgr, 0.6, hm_cm, 0.4, 0)\n",
    "    return cv2.cvtColor(mix, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def infer_conv_mode(model_name: str) -> str:\n",
    "    name = model_name.lower()\n",
    "    if \"llama-2\" in name:    return \"llava_llama_2\"\n",
    "    if \"mistral\" in name:    return \"mistral_instruct\"\n",
    "    if \"v1.6-34b\" in name:   return \"chatml_direct\"\n",
    "    if \"v1\" in name:         return \"llava_v1\"\n",
    "    if \"mpt\" in name:        return \"mpt\"\n",
    "    return \"llava_v0\"\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 모델 로드 및 초기화\n",
    "# =============================================================================\n",
    "print(\"▶ Loading LLaVA model...\")\n",
    "disable_torch_init()\n",
    "\n",
    "model_arch_name = get_model_name_from_path(MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR,\n",
    "    model_base=MODEL_BASE_LLM,\n",
    "    model_name=model_arch_name,\n",
    "    load_8bit=LOAD_8BIT,\n",
    "    load_4bit=LOAD_4BIT,\n",
    "    device=DEVICE\n",
    ")\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# 대화 템플릿 선택\n",
    "conv_key = CONV_MODE or infer_conv_mode(model_arch_name)\n",
    "if conv_key not in conv_templates:\n",
    "    raise ValueError(f\"Unknown conv mode: {conv_key}\")\n",
    "conv = conv_templates[conv_key].copy()\n",
    "roles = (\"user\",\"assistant\") if \"mpt\" in model_arch_name.lower() else conv.roles\n",
    "print(\"Conversation mode →\", conv_key)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 이미지 로드 & 전처리\n",
    "# =============================================================================\n",
    "pil_img = load_image(IMAGE_FILE_TO_PROCESS)\n",
    "print(\"Loaded image:\", pil_img.size)\n",
    "img_tensor = process_images([pil_img], image_processor, model.config)\n",
    "if isinstance(img_tensor, list):\n",
    "    img_tensor = [t.to(model.device, dtype=model.dtype) for t in img_tensor]\n",
    "else:\n",
    "    img_tensor = img_tensor.to(model.device, dtype=model.dtype)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. 프롬프트 구성\n",
    "# =============================================================================\n",
    "if model.config.mm_use_im_start_end:\n",
    "    user_inp = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN + DEFAULT_IM_END_TOKEN + \"\\n\" + USER_PROMPT\n",
    "else:\n",
    "    user_inp = DEFAULT_IMAGE_PATCH_TOKEN + \"\\n\" + USER_PROMPT\n",
    "\n",
    "conv.append_message(roles[0], user_inp)\n",
    "conv.append_message(roles[1], None)\n",
    "full_prompt = conv.get_prompt()\n",
    "\n",
    "print(\"\\n--- Prompt to tokenizer ---\")\n",
    "print(full_prompt)\n",
    "\n",
    "input_ids = tokenizer_image_token(\n",
    "    full_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
    ").unsqueeze(0).to(model.device)\n",
    "\n",
    "# =============================================================================\n",
    "# ★★★ 6. 텍스트 생성: 오직 여기만 수정 ★★★\n",
    "# =============================================================================\n",
    "# ① attention_mask 생성\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long().to(model.device)\n",
    "\n",
    "# ② generate() 호출 시 키 이름 변경 및 attention_mask 추가\n",
    "gen_kwargs = dict(\n",
    "    inputs=input_ids,               # 변경: input_ids=→ inputs=\n",
    "    attention_mask=attention_mask,  # 추가\n",
    "    images=img_tensor,\n",
    "    image_sizes=[pil_img.size],\n",
    "    do_sample=TEMPERATURE > 0,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True),\n",
    "    use_cache=True,\n",
    "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(f\"\\n[{roles[0]}] {USER_PROMPT}\")\n",
    "print(f\"[{roles[1]}] \", end=\"\", flush=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    model.generate(**gen_kwargs)\n",
    "\n",
    "print(\"\\n\\n--- Inference Complete. ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
