<file : llava-v1.5-7b-local/config.json>
{
  "_name_or_path": "llava-v1.5-7b",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "freeze_mm_vision_resampler": false,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "image_aspect_ratio": "pad",
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_projector_type": "mlp2x_gelu",
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "./models/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors",
  "model_type": "llava",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "unfreeze_mm_vision_tower": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

</file>
<file : llava/model/builder.py>
#    Copyright 2023 Haotian Liu
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.


import os
import warnings
import shutil

from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
import torch
from llava.model import *
from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.model.language_model.llava_llama import LlavaConfig


def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, load_4bit=False,
                          device_map="auto", device="cuda", use_flash_attn=False,
                          # torch_dtype을 **kwargs가 아닌 명시적 인자로 받도록 변경 (또는 kwargs에서 우선적으로 사용)
                          # 여기서는 **kwargs에 이미 torch_dtype이 들어온다고 가정하고,
                          # 초기 설정 부분을 수정합니다.
                          **kwargs): # **kwargs 안에 사용자가 전달한 torch_dtype이 있음
    
    # 사용자가 명시적으로 전달한 torch_dtype을_model_kwargs_torch_dtype 변수에 저장
    # 또는 함수 시그니처에 torch_dtype_arg=None 추가 후,
    # effective_torch_dtype = torch_dtype_arg if torch_dtype_arg is not None else kwargs.get('torch_dtype')
    user_specified_torch_dtype = kwargs.get('torch_dtype', None) # 사용자가 전달한 값 (예: torch.bfloat16)

    # kwargs는 AutoModelForCausalLM.from_pretrained 등에 전달될 딕셔너리
    # device_map 설정은 그대로 둠
    model_kwargs = {"device_map": device_map, **kwargs} # 사용자의 **kwargs를 먼저 펼치고 device_map 추가

    if device != "cuda": # 이 부분은 원래 로직 유지
        model_kwargs['device_map'] = {"": device}

    if load_8bit:
        model_kwargs['load_in_8bit'] = True
        # 8bit 로드 시 torch_dtype은 bnb_4bit_compute_dtype 등과 관련되므로,
        # 사용자가 명시한 torch_dtype과 충돌하지 않도록 주의.
        # 보통 이 경우 model_kwargs['torch_dtype']을 설정하지 않거나 bnb 설정에 따름.
        if 'torch_dtype' in model_kwargs: del model_kwargs['torch_dtype'] # 명시적 torch_dtype 제거
    elif load_4bit:
        model_kwargs['load_in_4bit'] = True
        model_kwargs['quantization_config'] = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16 if user_specified_torch_dtype == torch.bfloat16 else torch.float16, # bf16이면 bf16 사용
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4'
        )
        if 'torch_dtype' in model_kwargs: del model_kwargs['torch_dtype'] # 명시적 torch_dtype 제거
    else:
        # 8bit/4bit가 아닐 때, 사용자가 명시한 torch_dtype 사용
        if user_specified_torch_dtype is not None:
            model_kwargs['torch_dtype'] = user_specified_torch_dtype
        else: # 사용자가 명시하지 않으면 기본 float16 (기존 로직)
            model_kwargs['torch_dtype'] = torch.float16
            user_specified_torch_dtype = torch.float16 # LlavaConfig에 전달하기 위해 업데이트

    if use_flash_attn: # 이 부분은 원래 로직 유지
        model_kwargs['attn_implementation'] = 'flash_attention_2'

    if 'llava' in model_name.lower():
        # Load LLaVA model
        if 'lora' in model_name.lower() and model_base is None:
            warnings.warn('There is `lora` in model name but no `model_base` is provided. If you are loading a LoRA model, please provide the `model_base` argument. Detailed instruction: https://github.com/haotian-liu/LLaVA#launch-a-model-worker-lora-weights-unmerged.')
        if 'lora' in model_name.lower() and model_base is not None:
            from llava.model.language_model.llava_llama import LlavaConfig
            lora_cfg_pretrained = LlavaConfig.from_pretrained(model_path)
            lora_cfg_pretrained.attention_dropout = 0.0
            lora_cfg_pretrained.rope_theta = 10000
            lora_cfg_pretrained.attention_bias = False
            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
            print('Loading LLaVA from base model...')
            model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=lora_cfg_pretrained, **kwargs)
            token_num, tokem_dim = model.lm_head.out_features, model.lm_head.in_features
            if model.lm_head.weight.shape[0] != token_num:
                model.lm_head.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))
                model.model.embed_tokens.weight = torch.nn.Parameter(torch.empty(token_num, tokem_dim, device=model.device, dtype=model.dtype))

            print('Loading additional LLaVA weights...')
            if os.path.exists(os.path.join(model_path, 'non_lora_trainables.bin')):
                non_lora_trainables = torch.load(os.path.join(model_path, 'non_lora_trainables.bin'), map_location='cpu')
            else:
                # this is probably from HF Hub
                from huggingface_hub import hf_hub_download
                def load_from_hf(repo_id, filename, subfolder=None):
                    cache_file = hf_hub_download(
                        repo_id=repo_id,
                        filename=filename,
                        subfolder=subfolder)
                    return torch.load(cache_file, map_location='cpu')
                non_lora_trainables = load_from_hf(model_path, 'non_lora_trainables.bin')
            non_lora_trainables = {(k[11:] if k.startswith('base_model.') else k): v for k, v in non_lora_trainables.items()}
            if any(k.startswith('model.model.') for k in non_lora_trainables):
                non_lora_trainables = {(k[6:] if k.startswith('model.') else k): v for k, v in non_lora_trainables.items()}
            model.load_state_dict(non_lora_trainables, strict=False)

            from peft import PeftModel
            print('Loading LoRA weights...')
            model = PeftModel.from_pretrained(model, model_path)
            print('Merging LoRA weights...')
            model = model.merge_and_unload()
            print('Model is loaded...')
        elif model_base is not None:
            # this may be mm projector only
            print('Loading LLaVA from base model...')
            if 'mpt' in model_name.lower():
                if not os.path.isfile(os.path.join(model_path, 'configuration_mpt.py')):
                    shutil.copyfile(os.path.join(model_base, 'configuration_mpt.py'), os.path.join(model_path, 'configuration_mpt.py'))
                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=True)
                cfg_pretrained = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
                model = LlavaMptForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=cfg_pretrained, **kwargs)
            else:
                tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)


                # 2. model_base에서 기본 Llama 모델의 전체 설정을 로드합니다.
                # 여기에는 attention_dropout과 같은 Llama의 모든 기본 필드가 포함됩니다.
                full_base_cfg = AutoConfig.from_pretrained(model_base)

                # 3. model_path (LLaVA 모델 경로)에서 LLaVA 관련 설정을 로드합니다.
                #    이것은 LlavaConfig 객체여야 합니다 (config.json의 model_type="llava_llama" 가정).
                from llava.model.language_model.llava_llama import LlavaConfig
                llava_specific_cfg = LlavaConfig.from_pretrained(model_path)

                # 4. 기본 Llama 설정에 LLaVA 특화 설정을 덮어씁니다.
                #    이렇게 하면 Llama 기본값 위에 LLaVA 변경/추가 사항이 적용됩니다.
                #    LlavaConfig가 LlamaConfig를 상속하므로, to_dict()로 변환 후 업데이트.
                config_dict_to_update = full_base_cfg.to_dict()
                config_dict_to_update.update(llava_specific_cfg.to_dict())
                
                # load_pretrained_model 함수에 전달된 torch_dtype (예: torch.bfloat16)을
                # LlavaConfig 생성 시 명시적으로 전달합니다.
                # LlavaConfig.__init__에서 이 값을 사용하여 내부 플래그(bf16, fp16) 및
                # self.torch_dtype (실제 torch.dtype 객체)을 설정합니다.
                # kwargs에서 torch_dtype을 가져와 사용합니다. load_pretrained_model의 **kwargs에 이미 포함되어 있음.
                # 여기서 config_dict_to_update['torch_dtype'] = kwargs.get('torch_dtype') 와 같이 설정합니다.
                # 만약 **kwargs에 torch_dtype이 없다면 (함수 시그니처에 명시적으로 있지만),
                # 명시적으로 torch_dtype 변수를 사용합니다.
                # 이 함수 시그니처에는 **kwargs 외에 명시적인 torch_dtype 인자가 없으므로,
                # **kwargs를 통해 전달된 torch_dtype 값을 사용해야 합니다.
                # (주의: load_pretrained_model의 원래 kwargs에는 torch_dtype이 이미 있습니다.
                #  BitsAndBytesConfig 관련 로직에서 kwargs['torch_dtype'] = torch.float16 로 설정되기도 함)
                #  가장 확실한 것은 load_pretrained_model 함수의 인자로 받은 torch_dtype을 사용하는 것입니다.
                #  함수 시그니처에 torch_dtype이 없으므로, kwargs에서 가져옵니다.
                config_dict_to_update['torch_dtype'] = kwargs.get('torch_dtype')

                # 5. 최종적으로 병합된 dict로부터 LlavaConfig 객체를 생성합니다.
                final_config_for_model = LlavaConfig(**config_dict_to_update)

                model = LlavaLlamaForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, config=final_config_for_model, **kwargs)


            mm_projector_weights = torch.load(os.path.join(model_path, 'mm_projector.bin'), map_location='cpu')
            mm_projector_weights = {k: v.to(torch.float16) for k, v in mm_projector_weights.items()}
            model.load_state_dict(mm_projector_weights, strict=False)
        else:
            if 'mpt' in model_name.lower():
                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
                model = LlavaMptForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)
            elif 'mistral' in model_name.lower():
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = LlavaMistralForCausalLM.from_pretrained(
                    model_path,
                    low_cpu_mem_usage=True,
                    **kwargs
                )
            else:
                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
                model = LlavaLlamaForCausalLM.from_pretrained(
                    model_path,
                    low_cpu_mem_usage=True,
                    **kwargs
                )
    else:
        # Load language model
        if model_base is not None:
            # PEFT model
            from peft import PeftModel
            tokenizer = AutoTokenizer.from_pretrained(model_base, use_fast=False)
            model = AutoModelForCausalLM.from_pretrained(model_base, low_cpu_mem_usage=True, **kwargs)
            print(f"Loading LoRA weights from {model_path}")
            model = PeftModel.from_pretrained(model, model_path)
            print(f"Merging weights")
            model = model.merge_and_unload()
            print('Convert to FP16...')
            model.to(torch.float16)
        else:
            use_fast = False
            if 'mpt' in model_name.lower():
                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, trust_remote_code=True, **kwargs)
            else:
                tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
                model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)

    image_processor = None

    if 'llava' in model_name.lower():
        mm_use_im_start_end = getattr(model.config, "mm_use_im_start_end", False)
        mm_use_im_patch_token = getattr(model.config, "mm_use_im_patch_token", True)
        if mm_use_im_patch_token:
            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)
        if mm_use_im_start_end:
            tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)
        model.resize_token_embeddings(len(tokenizer))

        vision_tower = model.get_vision_tower()
        if not vision_tower.is_loaded:
            vision_tower.load_model(device_map=device_map)
        if device_map != 'auto': # True if device_map is "cuda", "cuda:0", etc.
            # model_kwargs['torch_dtype'] holds the torch.dtype object (e.g., torch.bfloat16)
            # or a string that was resolved earlier. It's the most reliable source for
            # the intended dtype of the entire model system.
            correct_dtype_for_conversion = model_kwargs.get('torch_dtype')

            # Ensure it's a torch.dtype object if it was passed as a string initially
            # (though in your case, it's already a torch.dtype object from the script)
            if isinstance(correct_dtype_for_conversion, str):
                correct_dtype_for_conversion = getattr(torch, correct_dtype_for_conversion, None)
            
            # Fallback logic for safety, though it shouldn't be hit in your specific scenario
            if correct_dtype_for_conversion is None:
                warnings.warn(
                    "Vision tower dtype could not be determined from model_kwargs, attempting to use model.dtype or defaulting to float16.",
                    UserWarning
                )
                if hasattr(model, 'dtype') and isinstance(model.dtype, torch.dtype):
                    correct_dtype_for_conversion = model.dtype
                else:
                    correct_dtype_for_conversion = torch.float16 # Last resort default

            # Now, `correct_dtype_for_conversion` should be a valid torch.dtype.
            # `device_map` here is a string like "cuda" or "cuda:0".
            # vision_tower.to() will move the nn.Module and its parameters.
            vision_tower.to(device=device_map, dtype=correct_dtype_for_conversion)
        image_processor = vision_tower.image_processor

    if hasattr(model.config, "max_sequence_length"):
        context_len = model.config.max_sequence_length
    else:
        context_len = 2048

    return tokenizer, model, image_processor, context_len
</file>
<file : llava/model/multimodal_encoder/builder.py>
# llava/model/multimodal_encoder/builder.py
import os
from .clip_encoder import CLIPVisionTower

def build_vision_tower(vision_tower_cfg, training_args=None, **kwargs):
    """
    vision_tower_cfg : ModelArguments 인스턴스
    training_args    : transformers.TrainingArguments 인스턴스 (bf16, fp16, device_map 등을 포함)
    kwargs           : delay_load 등 추가 옵션
    """
    # 1) 모델 인자에 fp16/bf16/device_map 주입
    if training_args is not None:
        setattr(vision_tower_cfg, 'fp16', getattr(training_args, 'fp16', False))
        setattr(vision_tower_cfg, 'bf16', getattr(training_args, 'bf16', False))
        setattr(vision_tower_cfg, 'device_map', getattr(training_args, 'device_map', None))

    # 2) 실제 vision_tower 문자열(경로 또는 HF ID) 확보
    vision_tower = getattr(
        vision_tower_cfg,
        'mm_vision_tower',
        getattr(vision_tower_cfg, 'vision_tower', None)
    )
    if vision_tower is None:
        raise ValueError("`vision_tower_cfg` 에 vision_tower 경로가 정의돼 있지 않습니다.")

    is_existing_path = os.path.exists(vision_tower)

    # 3) Reg-Gated, OpenAI, LAION, ShareGPT4V 모델만 지원
    if (
        is_existing_path
        or vision_tower.startswith("openai")
        or vision_tower.startswith("laion")
        or "ShareGPT4V" in vision_tower
        or "GATED" in vision_tower.upper()
    ):
        # CLIPVisionTower 시그니처: (vision_tower_path_or_name, args, delay_load=False)
        return CLIPVisionTower(
            vision_tower,        # 경로 또는 HF 모델명
            vision_tower_cfg,    # 업데이트된 ModelArguments (args.fp16, args.bf16 포함)
            **kwargs             # delay_load 등 추가 옵션
        )

    raise ValueError(f"Unknown vision tower: {vision_tower}")

</file>
<file : llava/model/language_model/llava_llama.py>
#    Copyright 2023 Haotian Liu
#
#    Licensed under the Apache License, Version 2.0 (the "License");
#    you may not use this file except in compliance with the License.
#    You may obtain a copy of the License at
#
#        http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS,
#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#    See the License for the specific language governing permissions and
#    limitations under the License.


from typing import List, Optional, Tuple, Union

import torch
import torch.nn as nn

from transformers import AutoConfig, AutoModelForCausalLM, \
                         LlamaConfig, LlamaModel, LlamaForCausalLM

from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers.generation.utils import GenerateOutput

from ..llava_arch import LlavaMetaModel, LlavaMetaForCausalLM


class LlavaConfig(LlamaConfig):
    model_type = "llava_llama"

    def __init__(
        self,
        # LLaMA 기본 인자들 (LlamaConfig에서 상속)
        # attention_dropout은 LlamaConfig의 기본값(0.0)을 따르거나, 여기서 명시적으로 설정 가능
        # super().__init__에 전달되므로, LlamaConfig가 처리함.

        # LLaVA 특화 인자들 (mm_projector, vision_tower 등)
        mm_vision_tower=None,
        mm_hidden_size=None, # 예시: projector 입력 차원 (vision tower 출력)
        mm_vision_select_layer=-2, # CLIPVisionTower에서 사용
        mm_vision_select_feature='patch', # CLIPVisionTower에서 사용
        mm_projector_type='linear', # projector 타입
        # ... 기타 LLaVA 관련 설정 ...

        # 커스텀 비전 인코더 및 경로 관리를 위한 인자들 (이전 논의에서 추가)
        model_dir_for_llava_parts=None, # LLaVA 모델 (프로젝터 등) 파일이 있는 디렉토리
        vision_encoder_base_dir=None,   # 커스텀 비전 인코더 safetensors 파일의 기본 디렉토리
        custom_image_mean=None,         # 커스텀 전처리기 평균값
        custom_image_std=None,          # 커스텀 전처리기 표준편차
        vision_image_size=224,          # 비전 타워 기본 이미지 크기 (CLIPVisionTower의 num_patches_per_side 계산용)
        vision_patch_size=14,           # 비전 타워 기본 패치 크기 (CLIPVisionTower의 num_patches_per_side 계산용)


        # 데이터 타입 관련 인자 (중요)
        torch_dtype=None, # "float16", "bfloat16", "float32" 또는 torch.dtype 객체
        
        **kwargs,
    ):
        # LLaVA 특화 인자들을 self에 먼저 할당
        self.mm_vision_tower = mm_vision_tower
        self.mm_hidden_size = mm_hidden_size
        self.mm_vision_select_layer = mm_vision_select_layer
        self.mm_vision_select_feature = mm_vision_select_feature
        self.mm_projector_type = mm_projector_type

        self.model_dir_for_llava_parts = model_dir_for_llava_parts
        self.vision_encoder_base_dir = vision_encoder_base_dir
        
        # custom_image_mean/std는 기본값을 여기서 설정하거나, 사용하는 곳에서 getattr으로 처리
        self.custom_image_mean = custom_image_mean if custom_image_mean is not None else [0.48145466, 0.4578275, 0.40821073]
        self.custom_image_std = custom_image_std if custom_image_std is not None else [0.26862954, 0.26130258, 0.27577711]
        self.vision_image_size = vision_image_size
        self.vision_patch_size = vision_patch_size

        # torch_dtype 처리 및 fp16/bf16 플래그 설정
        # LlamaConfig는 torch_dtype 인자를 직접 받으므로, kwargs에 추가/수정하여 전달
        # 또한, self.fp16, self.bf16 플래그도 설정하여 CLIPVisionTower 등에서 사용 가능하도록 함
        self.fp16 = False
        self.bf16 = False
        
        _actual_torch_dtype = None # LlamaConfig에 전달될 실제 torch.dtype 객체

        if torch_dtype is not None:
            if isinstance(torch_dtype, str):
                if torch_dtype.lower() == "float16":
                    self.fp16 = True
                    _actual_torch_dtype = torch.float16
                elif torch_dtype.lower() == "bfloat16":
                    self.bf16 = True
                    _actual_torch_dtype = torch.bfloat16
                elif torch_dtype.lower() == "float32":
                    _actual_torch_dtype = torch.float32
                # 다른 문자열 dtype 처리 (필요하다면)
            elif isinstance(torch_dtype, torch.dtype):
                if torch_dtype == torch.float16: self.fp16 = True
                elif torch_dtype == torch.bfloat16: self.bf16 = True
                _actual_torch_dtype = torch_dtype
        
        if _actual_torch_dtype is not None:
            kwargs['torch_dtype'] = _actual_torch_dtype # LlamaConfig가 받을 수 있도록 kwargs에 설정
        
        # LlamaConfig의 __init__ 호출
        # attention_dropout은 LlamaConfig에서 기본값이 0.0이므로, 명시적으로 전달하지 않아도 됨
        # 만약 다른 값을 원한다면 kwargs에 포함시키거나 직접 전달
        # super().__init__(attention_dropout=attention_dropout, **kwargs) # 원래 코드
        super().__init__(**kwargs) # 수정: LlamaConfig가 attention_dropout을 kwargs로 받도록 함 (또는 직접 명시)

        # super().__init__ 호출 후, torch_dtype이 설정되었는지 다시 확인하고,
        # self.torch_dtype (실제 torch.dtype 객체)을 명시적으로 저장할 수 있음.
        # LlamaConfig는 내부적으로 _torch_dtype을 설정하지만, 직접 접근 가능한 self.torch_dtype을 두는 것이 편할 수 있음.
        if hasattr(self, '_torch_dtype') and self._torch_dtype is not None: # LlamaConfig가 설정한 _torch_dtype
             self.torch_dtype = self._torch_dtype
        elif _actual_torch_dtype is not None:
             self.torch_dtype = _actual_torch_dtype
        else: # 기본값 (LlamaConfig의 기본값 또는 시스템 기본값)
             self.torch_dtype = torch.float32 # 또는 torch.get_default_dtype()

        # 만약 LlamaConfig가 fp16, bf16 플래그를 torch_dtype에 따라 자동으로 설정하지 않는다면, 여기서 명시적으로 설정
        # (LlamaConfig는 보통 torch_dtype에 따라 내부적으로 처리함)
        # self.fp16 = (self.torch_dtype == torch.float16)
        # self.bf16 = (self.torch_dtype == torch.bfloat16)



class LlavaLlamaModel(LlavaMetaModel, LlamaModel):
    config_class = LlavaConfig

    def __init__(self, config: LlavaConfig):
        super(LlavaLlamaModel, self).__init__(config)


class LlavaLlamaForCausalLM(LlamaForCausalLM, LlavaMetaForCausalLM):
    config_class = LlavaConfig

    def __init__(self, config: LlavaConfig): # Changed type hint to LlavaConfig
        super(LlamaForCausalLM, self).__init__(config)
        self.model = LlavaLlamaModel(config)

        # self.pretraining_tp and self.vocab_size are likely handled by LlamaForCausalLM's __init__
        # If LlamaConfig (and thus LlavaConfig) has pretraining_tp, it should be fine.
        # Verify if LlamaForCausalLM's __init__ correctly sets these based on the config.
        # If not, uncomment and ensure they are correctly sourced from config.
        # self.pretraining_tp = config.pretraining_tp 
        # self.vocab_size = config.vocab_size

        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # Initialize weights and apply final processing
        self.post_init()

    def get_model(self):
        return self.model

    # NOTE: HF generate() (4.30↑) → forward(cache_position=…)
    #       받아서 무시하거나 상위로 전달해야 에러가 나지 않는다.
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        images: Optional[torch.FloatTensor] = None,
        image_sizes: Optional[List[List[int]]] = None,
        return_dict: Optional[bool] = None,
        cache_position: Optional[torch.LongTensor] = None,
        **kwargs,
    ) -> Union[Tuple, CausalLMOutputWithPast]:

        if inputs_embeds is None:
            (
                input_ids,
                position_ids,
                attention_mask,
                past_key_values,
                inputs_embeds,
                labels
            ) = self.prepare_inputs_labels_for_multimodal(
                input_ids,
                position_ids,
                attention_mask,
                past_key_values,
                labels,
                images,
                image_sizes
            )

        return super().forward(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )

    @torch.no_grad()
    def generate(
        self,
        inputs: Optional[torch.Tensor] = None,
        images: Optional[torch.Tensor] = None,
        image_sizes: Optional[torch.Tensor] = None,
        **kwargs,
    ) -> Union[GenerateOutput, torch.LongTensor]:
        # Pop position_ids and attention_mask from kwargs.
        # If they are not provided (i.e., None), the base model's generate/forward will handle them.
        position_ids = kwargs.pop("position_ids", None)
        attention_mask = kwargs.pop("attention_mask", None)

        if "inputs_embeds" in kwargs:
            raise NotImplementedError("`inputs_embeds` is not supported when calling `generate` directly with `inputs_embeds`.")

        if images is not None:
            # When images are present, inputs_embeds are prepared by prepare_inputs_labels_for_multimodal.
            # This function also returns potentially modified position_ids and attention_mask.
            (
                prepared_input_ids, # This should be None if inputs_embeds is returned
                prepared_position_ids,
                prepared_attention_mask,
                _, # past_key_values
                inputs_embeds, # This is the primary input to the LLM
                _  # labels
            ) = self.prepare_inputs_labels_for_multimodal(
                inputs, # Original input_ids (text tokens)
                position_ids, # Original position_ids from kwargs
                attention_mask, # Original attention_mask from kwargs
                None, # past_key_values for prefill stage
                None, # labels for prefill stage
                images,
                image_sizes=image_sizes
            )
            # Ensure the prepared position_ids and attention_mask are on the same device as inputs_embeds.
            # inputs_embeds should be on the correct model device after prepare_inputs_labels_for_multimodal.
            final_position_ids = prepared_position_ids.to(inputs_embeds.device) if prepared_position_ids is not None else None
            final_attention_mask = prepared_attention_mask.to(inputs_embeds.device) if prepared_attention_mask is not None else None

        else:
            # No images, standard text generation.
            # inputs are token IDs. We need to embed them.
            if inputs is None:
                raise ValueError("`inputs` (input_ids) must be provided if `images` is None.")

            # Determine the target device from the model's embedding layer.
            target_device = self.get_model().embed_tokens.weight.device
            inputs = inputs.to(target_device) # Ensure input_ids are on the correct device.
            inputs_embeds = self.get_model().embed_tokens(inputs)

            # Use position_ids and attention_mask popped from kwargs, ensuring they are on the correct device.
            final_position_ids = position_ids.to(inputs_embeds.device) if position_ids is not None else None
            final_attention_mask = attention_mask.to(inputs_embeds.device) if attention_mask is not None else None

        # Pass inputs_embeds to the base model's generate method.
        # input_ids should ideally be None if inputs_embeds is used,
        # but Hugging Face generate handles this.
        return super().generate(
            # input_ids=None, # Explicitly set to None if not needed by superclass when inputs_embeds is present
            position_ids=final_position_ids,
            attention_mask=final_attention_mask,
            inputs_embeds=inputs_embeds,
            **kwargs
        )

    def prepare_inputs_for_generation(self, input_ids, past_key_values=None,
                                      inputs_embeds=None, **kwargs):
        images = kwargs.pop("images", None)
        image_sizes = kwargs.pop("image_sizes", None)
        inputs = super().prepare_inputs_for_generation(
            input_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, **kwargs
        )
        if images is not None:
            inputs['images'] = images
        if image_sizes is not None:
            inputs['image_sizes'] = image_sizes
        return inputs

AutoConfig.register("llava_llama", LlavaConfig)
AutoModelForCausalLM.register(LlavaConfig, LlavaLlamaForCausalLM)

</file>
<file : INFERclipregXGATED/model.py>
from collections import OrderedDict
from typing import Tuple, Union

import numpy as np
import os
import torch
import torch.nn.functional as F
from torch import nn
from transformers.modeling_outputs import BaseModelOutputWithPooling

class LayerNorm(torch.nn.LayerNorm):
    """
    Subclass torch's LayerNorm to handle fp16/bf16.
    """
    def forward(self, x):
        # Store original dtype
        orig_dtype = x.dtype
        orig_x = x
        
        # Check if parameters are in a different dtype than float32
        if self.weight.dtype != torch.float32:
            # Cast weights and bias to float32 temporarily
            weight = self.weight.to(torch.float32)
            bias = self.bias.to(torch.float32) if self.bias is not None else None
            
            # Apply layer norm manually to avoid dtype issues
            x = x.to(torch.float32)
            mean = x.mean(-1, keepdim=True)
            var = x.var(-1, unbiased=False, keepdim=True)
            normalized = (x - mean) / torch.sqrt(var + self.eps)
            
            if weight is not None and bias is not None:
                output = normalized * weight + bias
            elif weight is not None:
                output = normalized * weight
            else:
                output = normalized
        else:
            # If weights are already float32, use the parent implementation
            x = x.to(torch.float32)
            output = super().forward(x)
        
        # Return with original dtype
        return output.to(orig_dtype)


class QuickGELU(nn.Module):
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)


class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        attn_mask = None

        if self.attn_mask is not None:
            n_ctx = x.shape[0]
            attn_mask = self.attn_mask[..., -n_ctx:, -n_ctx:].to(dtype=x.dtype, device=x.device)            

        return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x

class Transformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])

    def forward(self, x: torch.Tensor):
        return self.resblocks(x)


# This class integrates the intermediate gating mechanism into the vision transformer.
class VitTransformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, gate_start_layer: int, num_registers: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.gate_start_layer = gate_start_layer
        self.num_registers = num_registers

        # Use ModuleList to allow non-inplace updates and per-layer operations.
        self.resblocks = nn.ModuleList([ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])
        
        if layers >= gate_start_layer:
            self.intermediate_fusion_mlps = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(2 * width, width),
                    nn.ReLU(),
                    nn.Linear(width, 1)
                ) for _ in range(layers - gate_start_layer + 1)
            ])
            # Initialize each intermediate gating MLP
            for gate in self.intermediate_fusion_mlps:
                for m in gate.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.xavier_uniform_(m.weight)
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
        else:
            self.intermediate_fusion_mlps = None

    def forward(self, x: torch.Tensor, output_hidden_states: bool = False):
        # x shape: [seq_len, batch, width]
        all_hidden_states = () if output_hidden_states else None # 중간 출력 저장용 튜플

        for i, block in enumerate(self.resblocks):
            x = block(x) # 각 블록 통과

            # --- 게이팅 로직 적용 (블록 출력 후) ---
            # 게이팅은 CLS 토큰을 변경하므로, 저장 시점 중요.
            # HF 관례는 보통 레이어(블록) 출력 직후 저장. 게이팅은 레이어 이후의 추가 처리로 간주.
            # 따라서 게이팅 *전*의 x를 저장하는 것이 일반적일 수 있으나,
            # 여기서는 게이팅이 CLS 토큰 자체를 업데이트하므로, 업데이트 *후*의 상태가
            # 다음 레이어의 입력에 더 가까울 수 있음. 여기서는 게이팅 *후*의 x를 저장.
            if self.intermediate_fusion_mlps is not None and (i + 1) >= self.gate_start_layer:
                cls_token = x[0]                     # [batch, width]
                reg_tokens = x[1:1+self.num_registers] # [num_registers, batch, width]
                reg_summary = reg_tokens.mean(dim=0)   # [batch, width]
                fusion_input = torch.cat([cls_token, reg_summary], dim=-1)  # [batch, 2*width]
                gate_index = (i + 1) - self.gate_start_layer  # index into intermediate_fusion_mlps
                gate = torch.sigmoid(self.intermediate_fusion_mlps[gate_index](fusion_input))  # [batch, 1]
                fused = gate * cls_token + (1 - gate) * reg_summary  # fused representation
                # Instead of an in-place update, create a new tensor for x:
                x = torch.cat([fused.unsqueeze(0), x[1:]], dim=0) # 게이팅 적용된 x

            # --- 중간 출력 저장 ---
            if output_hidden_states:
                # 현재 레이어(게이팅 포함)를 통과한 후의 상태 저장
                all_hidden_states = all_hidden_states + (x,) # 튜플에 추가

        if output_hidden_states:
            return x, all_hidden_states # 최종 출력과 중간 출력 튜플 반환
        else:
            return x # 최종 출력만 반환



# For CLIP-REG GATED model
class VisionTransformer(nn.Module):
    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int, num_registers: int = 4):
        super().__init__()
        self.input_resolution = input_resolution
        self.patch_size = patch_size # <-- 추가: patch_size 저장
        self.output_dim = output_dim # output_dim은 정의되지만, LLaVA에서는 proj를 거치지 않음
        self.num_registers = num_registers

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)

        scale = width ** -0.5
        self.class_embedding = nn.Parameter(scale * torch.randn(width))

        num_patches = (input_resolution // patch_size) ** 2
        num_tokens = num_patches + 1 + num_registers
        self.positional_embedding = nn.Parameter(scale * torch.randn(num_tokens, width))

        self.ln_pre = LayerNorm(width)

        self.register_tokens = nn.Parameter(torch.empty(num_registers, width)) # build_model에서 실제 값 로드

        self.transformer = VitTransformer(width, layers, heads, gate_start_layer=14, num_registers=num_registers)

        self.ln_post = LayerNorm(width)
        # self.proj 는 LLaVA 연동 시 사용되지 않음.
        # 가중치 로드 시 strict=False를 사용하므로, state_dict에 있더라도 모델 구조상 없어도 됨.
        # 만약 strict=True 로 로드해야 하거나, 다른 용도로 필요하다면 정의는 유지.
        # 여기서는 LLaVA 사용에 집중하므로 주석 처리하거나 삭제 가능 (단, build_model에서 strict=False 확인 필요)
        # self.proj = nn.Parameter(scale * torch.randn(width, output_dim))

        # 최종 fusion_mlp 는 LLaVA 출력에 필요 없으므로 주석 처리 또는 삭제
        # self.fusion_mlp = nn.Sequential(...)


    def forward(self, x: torch.Tensor, output_hidden_states=False):
        x = self.conv1(x)
        x = x.reshape(x.shape[0], x.shape[1], -1).permute(0, 2, 1) # [B, num_patches, width]

        cls_token = self.class_embedding.to(x.dtype).expand(x.shape[0], 1, -1) # [B, 1, width]
        register_tokens = self.register_tokens.to(x.dtype).unsqueeze(0).expand(x.shape[0], -1, -1) # [B, num_reg, width]

        x = torch.cat([cls_token, register_tokens, x], dim=1) # [B, SeqLen, width]
        x = x + self.positional_embedding.to(x.dtype)

        # --- ln_pre 적용 전 상태 저장 (선택적, HF는 보통 임베딩 출력 포함) ---
        # hidden_states_before_ln_pre = x if output_hidden_states else None

        x = self.ln_pre(x) # [B, SeqLen, H]

        # --- 초기 입력 상태 저장 (ln_pre 후) ---
        # 이게 hidden_states의 첫번째 요소가 됨 (HF 관례)
        initial_hidden_state = x if output_hidden_states else None

        x = x.permute(1, 0, 2)  # shape: [seq_len, batch, width]

        # --- VitTransformer 호출 시 output_hidden_states 전달 ---
        transformer_outputs = self.transformer(x, output_hidden_states=output_hidden_states)

        # --- 결과 처리 ---
        if output_hidden_states:
            # 최종 블록 출력과 중간 블록 출력들 분리
            final_block_output = transformer_outputs[0] # [SeqLen, B, H]
            intermediate_hidden_states_unpermuted = transformer_outputs[1] # 튜플: 각 요소 [SeqLen, B, H]
        else:
            final_block_output = transformer_outputs # [SeqLen, B, H]
            intermediate_hidden_states_unpermuted = None

        # --- 최종 블록 출력을 원래 차원으로 복원 ---
        x_final_permuted = final_block_output.permute(1, 0, 2)  # shape: [batch, seq_len, width]

        # --- 최종 LayerNorm 적용 ---
        last_hidden_state = self.ln_post(x_final_permuted) # [B, SeqLen, 1024]

        # --- Pooler 출력 계산 ---
        pooled_output = last_hidden_state[:, 0] # CLS token [B, 1024]

        # --- hidden_states 튜플 구성 ---
        all_hidden_states = None
        if output_hidden_states:
            # 1. 초기 입력 상태 (ln_pre 후)
            all_hidden_states = (initial_hidden_state,)

            # 2. 중간 블록 출력들 (차원 복원)
            # intermediate_hidden_states_unpermuted는 None일 수 있으므로 체크
            if intermediate_hidden_states_unpermuted is not None:
                for hidden_state_unpermuted in intermediate_hidden_states_unpermuted:
                    hidden_state_permuted = hidden_state_unpermuted.permute(1, 0, 2) # [B, SeqLen, H]
                    all_hidden_states = all_hidden_states + (hidden_state_permuted,)

            # 3. 최종 출력 (ln_post 후) - HF는 보통 이것도 포함하지만, LLaVA는 last_hidden_state를 따로 쓰므로 생략 가능
            # all_hidden_states = all_hidden_states + (last_hidden_state,) # 필요 시 이 라인 활성화

        return BaseModelOutputWithPooling(
            last_hidden_state=last_hidden_state, # 최종 출력 (ln_post 후)
            pooler_output=pooled_output,
            hidden_states=all_hidden_states    # 구성된 중간 출력 튜플 (또는 None)
        )



class CLIP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 # vision
                 image_resolution: int,
                 vision_layers: Union[Tuple[int, int, int, int], int],
                 vision_width: int,
                 vision_patch_size: int,
                 num_registers: int, # <-- 추가
                 # text
                 context_length: int,
                 vocab_size: int,
                 transformer_width: int,
                 transformer_heads: int,
                 transformer_layers: int
                 ):
        super().__init__()

        self.context_length = context_length

        vision_heads = vision_width // 64
        self.visual = VisionTransformer( # num_registers 전달
            input_resolution=image_resolution,
            patch_size=vision_patch_size,
            width=vision_width,
            layers=vision_layers,
            heads=vision_heads,
            output_dim=embed_dim, # output_dim 전달
            num_registers=num_registers # <-- 추가
        )
        # ... (텍스트 부분 초기화) ...
        self.transformer = Transformer( # Transformer 초기화 추가 (기존 코드에 있었어야 함)
             width=transformer_width,
             layers=transformer_layers,
             heads=transformer_heads,
             attn_mask=self.build_attention_mask()
        )
        self.vocab_size = vocab_size # 추가 (기존 코드에 있었어야 함)
        self.token_embedding = nn.Embedding(vocab_size, transformer_width) # 추가
        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width)) # 추가
        self.ln_final = LayerNorm(transformer_width) # 추가
        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim)) # 추가
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)) # 추가

        self.initialize_parameters() # 추가 (기존 코드에 있었어야 함)

    def initialize_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.positional_embedding, std=0.01)

        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)


        if self.text_projection is not None:
            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    def build_attention_mask(self):
        # lazily create causal attention mask, with full attention between the vision tokens
        mask = torch.empty(self.context_length, self.context_length)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal

        return mask


    @property
    def dtype(self):
        return self.visual.conv1.weight.dtype

    def encode_image(self, image):
        return self.visual(image.type(self.dtype))

    def encode_text(self, text):
        n_ctx = text.shape[-1]
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding[:n_ctx].type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

        return x

    def forward(self, image, text):
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)

        # normalized features
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # cosine similarity as logits
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()

        # shape = [global_batch_size, global_batch_size]
        return logits_per_image, logits_per_text


def convert_weights(model: nn.Module, target_dtype: torch.dtype): # 기본값 제거, 명시적 전달 강제
    """Convert applicable model parameters and buffers to a target_dtype"""
    print(f"  Attempting to convert model weights to {target_dtype}...")
    conversion_count = 0

    def _convert_recursive(module: nn.Module):
        nonlocal conversion_count
        # 먼저 자식 모듈에 대해 재귀적으로 호출
        for child_name, child_module in module.named_children():
            _convert_recursive(child_module)

        # 현재 모듈의 직접적인 파라미터 변환
        for param_name, param in module.named_parameters(recurse=False): # recurse=False로 직접 파라미터만
            if param.dtype != target_dtype:
                try:
                    param.data = param.data.to(dtype=target_dtype)
                    if param.grad is not None and param.grad.dtype != target_dtype:
                        param.grad.data = param.grad.data.to(dtype=target_dtype)
                    # print(f"    Converted parameter '{param_name}' to {target_dtype}")
                    conversion_count += 1
                except Exception as e:
                    print(f"    Error converting parameter '{param_name}': {e}")
        
        # 현재 모듈의 직접적인 버퍼 변환
        for buffer_name, buffer_tensor in module.named_buffers(recurse=False): # recurse=False로 직접 버퍼만
            if buffer_tensor.dtype != target_dtype and buffer_tensor.is_floating_point(): # 부동소수점 버퍼만 변환
                try:
                    # 버퍼는 _buffers 딕셔너리를 통해 직접 재할당
                    module._buffers[buffer_name] = buffer_tensor.to(dtype=target_dtype)
                    # print(f"    Converted buffer '{buffer_name}' to {target_dtype}")
                    conversion_count +=1
                except Exception as e:
                    print(f"    Error converting buffer '{buffer_name}': {e}")

    _convert_recursive(model) # model.apply 대신 재귀 함수 사용으로 로그 상세화 가능
    # model.apply(_convert_module_params_buffers) # 또는 apply 사용 유지
    if conversion_count == 0:
        print(f"  No weights needed conversion to {target_dtype}, or conversion failed silently for some.")
    else:
        print(f"  Successfully converted {conversion_count} parameters/buffers to {target_dtype}.")


def adjust_state_dict(state_dict, regtoken_path="regtokens"):
    new_state_dict = {}

    # Expand Positional Embedding if Needed
    old_pos_embed = state_dict["visual.positional_embedding"]
    old_size = old_pos_embed.shape[0]
    new_size = 261 #old_size + 4  # # 256 [VIS] + 1 [CLS] + 4 [REG] tokens

    if old_size != new_size:
        print(f"[---! INFO !---] Expanding positional embedding from {old_size} → {new_size}")

        # Initialize new [REG] positions using mean of existing embeddings
        mean_embedding = old_pos_embed.mean(dim=0, keepdim=True)
        expanded_embedding = torch.cat([old_pos_embed, mean_embedding.repeat(4, 1)], dim=0)

        new_state_dict["visual.positional_embedding"] = expanded_embedding
    else:
        print("[---! OK !---] Positional embedding size is already correct, skipping expansion.")

    # Inject Register Tokens: Load if available, otherwise initialize empty
    if "visual.register_tokens" not in state_dict:
        print("[---! INFO !---] Register tokens missing, using torch.empty placeholder.")
        reg_tokens = []
        for i in range(1, 5):
            reg_tokens.append(torch.empty(1024, dtype=torch.float32))
            
        new_state_dict["visual.register_tokens"] = torch.stack(reg_tokens)
    else:
        print("[---! OK !---] [REG] tokens already present, skipping injection.")

    return new_state_dict



# Modify the build_model function to adjust the state_dict before loading it
def build_model(state_dict: dict, regtoken_path="regtokens", model_dtype: torch.dtype = torch.float16):
    # ... (기존 model 구조 및 파라미터 계산 로직은 동일) ...
    vision_width = state_dict["visual.conv1.weight"].shape[0]
    layer_indices = set()
    for k in state_dict.keys():
        if k.startswith("visual.transformer.resblocks.") and k.endswith(".attn.in_proj_weight"):
            try:
                layer_indices.add(int(k.split('.')[3]))
            except (IndexError, ValueError):
                continue
    vision_layers = len(layer_indices)
    if vision_layers == 0:
         vision_layers = len([k for k in state_dict.keys() if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")]) # fallback

    vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
    if "visual.register_tokens" in state_dict:
        num_registers = state_dict["visual.register_tokens"].shape[0]
    else:
        num_registers = 4 # 기본값 또는 다른 추론 로직
    print(f"Determined num_registers: {num_registers}")

    num_tokens = state_dict["visual.positional_embedding"].shape[0]
    num_patches_calc = num_tokens - 1 - num_registers
    if num_patches_calc <= 0:
         raise ValueError(f"Calculated non-positive number of patches ({num_patches_calc})")
    grid_size = round(num_patches_calc ** 0.5)
    if grid_size * grid_size != num_patches_calc:
         grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5) # fallback

    image_resolution = vision_patch_size * grid_size
    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict if k.startswith("transformer.resblocks")))

    model = CLIP(
        embed_dim=embed_dim, image_resolution=image_resolution, vision_layers=vision_layers,
        vision_width=vision_width, vision_patch_size=vision_patch_size, num_registers=num_registers,
        context_length=context_length, vocab_size=vocab_size, transformer_width=transformer_width,
        transformer_heads=transformer_heads, transformer_layers=transformer_layers
    )

    for key_to_del in ["input_resolution", "context_length", "vocab_size"]: # 오타 수정
        if key_to_del in state_dict:
            del state_dict[key_to_del]

    print("Loading state_dict into CLIP model (strict=False)...")
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    if missing_keys:
        print(f"  Missing keys: {missing_keys}")
    if unexpected_keys:
        print(f"  Unexpected keys: {unexpected_keys}")

    print(f"Data type of model.visual.conv1.weight BEFORE explicit convert_weights: {model.visual.conv1.weight.dtype}")
    # model_dtype 인자가 torch.bfloat16으로 전달될 것임
    convert_weights(model, target_dtype=model_dtype) 
    print(f"Data type of model.visual.conv1.weight AFTER explicit convert_weights: {model.visual.conv1.weight.dtype}")

    return model.eval()

</file>
<file : llava/mm_utils.py>
from PIL import Image
from io import BytesIO
import base64
import os
from typing import List
import torch
from torchvision.transforms import Compose
import math
import ast

from transformers import StoppingCriteria
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN


def select_best_resolution(original_size, possible_resolutions):
    """
    Selects the best resolution from a list of possible resolutions based on the original size.

    Args:
        original_size (tuple): The original size of the image in the format (width, height).
        possible_resolutions (list): A list of possible resolutions in the format [(width1, height1), (width2, height2), ...].

    Returns:
        tuple: The best fit resolution in the format (width, height).
    """
    original_width, original_height = original_size
    best_fit = None
    max_effective_resolution = 0
    min_wasted_resolution = float('inf')

    for width, height in possible_resolutions:
        scale = min(width / original_width, height / original_height)
        downscaled_width, downscaled_height = int(original_width * scale), int(original_height * scale)
        effective_resolution = min(downscaled_width * downscaled_height, original_width * original_height)
        wasted_resolution = (width * height) - effective_resolution

        if effective_resolution > max_effective_resolution or (effective_resolution == max_effective_resolution and wasted_resolution < min_wasted_resolution):
            max_effective_resolution = effective_resolution
            min_wasted_resolution = wasted_resolution
            best_fit = (width, height)

    return best_fit


def resize_and_pad_image(image, target_resolution):
    """
    Resize and pad an image to a target resolution while maintaining aspect ratio.

    Args:
        image (PIL.Image.Image): The input image.
        target_resolution (tuple): The target resolution (width, height) of the image.

    Returns:
        PIL.Image.Image: The resized and padded image.
    """
    original_width, original_height = image.size
    target_width, target_height = target_resolution

    scale_w = target_width / original_width
    scale_h = target_height / original_height

    if scale_w < scale_h:
        new_width = target_width
        new_height = min(math.ceil(original_height * scale_w), target_height)
    else:
        new_height = target_height
        new_width = min(math.ceil(original_width * scale_h), target_width)

    # Resize the image
    resized_image = image.resize((new_width, new_height))

    new_image = Image.new('RGB', (target_width, target_height), (0, 0, 0))
    paste_x = (target_width - new_width) // 2
    paste_y = (target_height - new_height) // 2
    new_image.paste(resized_image, (paste_x, paste_y))

    return new_image


def divide_to_patches(image, patch_size):
    """
    Divides an image into patches of a specified size.

    Args:
        image (PIL.Image.Image): The input image.
        patch_size (int): The size of each patch.

    Returns:
        list: A list of PIL.Image.Image objects representing the patches.
    """
    patches = []
    width, height = image.size
    for i in range(0, height, patch_size):
        for j in range(0, width, patch_size):
            box = (j, i, j + patch_size, i + patch_size)
            patch = image.crop(box)
            patches.append(patch)

    return patches


def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):
    """
    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.

    Args:
        image_size (tuple): The size of the input image in the format (width, height).
        grid_pinpoints (str): A string representation of a list of possible resolutions.
        patch_size (int): The size of each image patch.

    Returns:
        tuple: The shape of the image patch grid in the format (width, height).
    """
    if type(grid_pinpoints) is list:
        possible_resolutions = grid_pinpoints
    else:
        possible_resolutions = ast.literal_eval(grid_pinpoints)
    width, height = select_best_resolution(image_size, possible_resolutions)
    return width // patch_size, height // patch_size


def process_anyres_image(image, processor, grid_pinpoints):
    """
    Process an image with variable resolutions.

    Args:
        image (PIL.Image.Image): The input image to be processed.
        processor: The image processor object.
        grid_pinpoints (str): A string representation of a list of possible resolutions.

    Returns:
        torch.Tensor: A tensor containing the processed image patches.
    """
    if type(grid_pinpoints) is list:
        possible_resolutions = grid_pinpoints
    else:
        possible_resolutions = ast.literal_eval(grid_pinpoints)
    best_resolution = select_best_resolution(image.size, possible_resolutions)
    image_padded = resize_and_pad_image(image, best_resolution)

    patches = divide_to_patches(image_padded, processor.crop_size['height'])

    image_original_resize = image.resize((processor.size['shortest_edge'], processor.size['shortest_edge']))

    image_patches = [image_original_resize] + patches
    image_patches = [processor.preprocess(image_patch, return_tensors='pt')['pixel_values'][0]
                     for image_patch in image_patches]
    return torch.stack(image_patches, dim=0)


def load_image_from_base64(image):
    return Image.open(BytesIO(base64.b64decode(image)))


def expand2square(pil_img, background_color):
    width, height = pil_img.size
    if width == height:
        return pil_img
    elif width > height:
        result = Image.new(pil_img.mode, (width, width), background_color)
        result.paste(pil_img, (0, (width - height) // 2))
        return result
    else:
        result = Image.new(pil_img.mode, (height, height), background_color)
        result.paste(pil_img, ((height - width) // 2, 0))
        return result


def process_images(images: List[Image.Image], image_processor, model_cfg): # images 타입 힌트 추가
    image_aspect_ratio = getattr(model_cfg, "image_aspect_ratio", None)
    new_images = []

    if image_aspect_ratio == 'pad':
        for image in images:
            background_color = tuple(int(x*255) for x in getattr(image_processor, 'image_mean', [0.48145466, 0.4578275, 0.40821073]))
            image = expand2square(image, background_color)
            
            if hasattr(image_processor, 'preprocess'): # 표준 HF ImageProcessor
                # preprocess는 보통 단일 이미지를 받고, 결과를 딕셔너리로 반환하며, 그 안에 'pixel_values'가 배치 차원 없이 있음
                processed_image = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]
            elif isinstance(image_processor, Compose): # torchvision.transforms.Compose 객체
                # Compose 객체는 보통 단일 이미지를 받아 단일 텐서(C, H, W)를 반환
                processed_image = image_processor(image)
                # reg_gated_transform이 (C,H,W) 텐서를 반환한다고 가정. 만약 다른 형태면 추가 처리 필요.
            else:
                raise TypeError(f"Unsupported image_processor type: {type(image_processor)}")
            new_images.append(processed_image)

    elif image_aspect_ratio == "anyres":
        for image in images:
            # process_anyres_image 내부에서도 image_processor 타입에 따른 분기 처리가 필요함
            # 여기서는 process_anyres_image가 이를 처리한다고 가정하고 호출
            # 또는 process_anyres_image에 image_processor가 Compose인지 여부를 전달할 수 있음
            processed_image = process_anyres_image(image, image_processor, model_cfg.image_grid_pinpoints, model_cfg) # model_cfg 전체 전달 고려
            new_images.append(processed_image)
            
    else: # 'pad'나 'anyres'가 아닌 경우 (예: None 또는 다른 값)
        if hasattr(image_processor, 'preprocess') and callable(getattr(image_processor, 'preprocess', None)):
            # 표준 HF ImageProcessor: 이미지 리스트를 받아 처리하고 'pixel_values'로 딕셔너리 반환 기대
            # (주의: HF ImageProcessor의 __call__ 메소드가 preprocess를 호출하는 경우가 많음)
            # 가장 안전한 것은 ImageProcessor의 __call__ 인터페이스를 따르는 것
            # image_processor(images, return_tensors='pt')는 보통 {'pixel_values': (B, C, H, W)}를 반환
            # 이 경우, return image_processor(images, return_tensors='pt')['pixel_values']가 맞음
            # 하지만 CLIPImageProcessor.__call__은 단일 이미지를 받아 pixel_values: (1,C,H,W) 를 반환 후 squeeze(0)
            # 만약 images가 리스트라면, 반복 처리 필요
            
            # 우선, image_processor가 리스트를 직접 처리할 수 있는지 확인 (HF ImageProcessor의 일반적인 사용법)
            try:
                # image_processor가 __call__ 메소드에서 리스트와 return_tensors를 지원한다고 가정
                return image_processor(images, return_tensors='pt')['pixel_values']
            except Exception: # 만약 실패하면, 개별 처리
                for image in images:
                    # 단일 이미지에 대한 preprocess 호출 (결과는 (C,H,W) 가정)
                    new_images.append(image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0])

        elif isinstance(image_processor, Compose): # torchvision.transforms.Compose 객체
            for image in images:
                # Compose 객체는 단일 이미지를 받아 단일 텐서(C, H, W)를 반환
                new_images.append(image_processor(image))
        else:
            raise TypeError(f"Unsupported image_processor type for general case: {type(image_processor)}")

    # 모든 이미지가 동일한 형태인지 확인 후 스택 (이 부분은 유지)
    if new_images and all(x.shape == new_images[0].shape for x in new_images):
        new_images_stacked = torch.stack(new_images, dim=0) # (B, C, H, W)
        return new_images_stacked
    elif not new_images: # 처리된 이미지가 없는 경우
        # 빈 리스트를 반환하거나, 빈 텐서를 반환하거나, 오류를 발생시킬 수 있음
        # 여기서는 빈 텐서를 반환하는 예시 (형태는 상황에 맞게 조정)
        # model_cfg에서 dtype 가져오기
        target_dtype = getattr(model_cfg, 'torch_dtype', torch.float32)
        return torch.empty((0, 3, getattr(image_processor, 'crop_size', {}).get('height', 224), getattr(image_processor, 'crop_size', {}).get('width', 224)), dtype=target_dtype)
    else: # 스택할 수 없는 경우 (이미지 크기가 다를 때) - 이 경우는 로직상 발생하면 안됨
        # 또는 리스트 그대로 반환 (LLaVA의 다른 부분이 리스트를 처리할 수 있다면)
        # raise ValueError("Processed images have different shapes and cannot be stacked.")
        return new_images # 혹은 오류 발생


def tokenizer_image_token(prompt, tokenizer, image_token_index=IMAGE_TOKEN_INDEX, return_tensors=None):
    prompt_chunks = [tokenizer(chunk).input_ids for chunk in prompt.split('<image>')]

    def insert_separator(X, sep):
        return [ele for sublist in zip(X, [sep]*len(X)) for ele in sublist][:-1]

    input_ids = []
    offset = 0
    if len(prompt_chunks) > 0 and len(prompt_chunks[0]) > 0 and prompt_chunks[0][0] == tokenizer.bos_token_id:
        offset = 1
        input_ids.append(prompt_chunks[0][0])

    for x in insert_separator(prompt_chunks, [image_token_index] * (offset + 1)):
        input_ids.extend(x[offset:])

    if return_tensors is not None:
        if return_tensors == 'pt':
            return torch.tensor(input_ids, dtype=torch.long)
        raise ValueError(f'Unsupported tensor type: {return_tensors}')
    return input_ids


def get_model_name_from_path(model_path):
    model_path = model_path.strip("/")
    model_paths = model_path.split("/")
    if model_paths[-1].startswith('checkpoint-'):
        return model_paths[-2] + "_" + model_paths[-1]
    else:
        return model_paths[-1]

class KeywordsStoppingCriteria(StoppingCriteria):
    def __init__(self, keywords, tokenizer, input_ids):
        self.keywords = keywords
        self.keyword_ids = []
        self.max_keyword_len = 0
        for keyword in keywords:
            cur_keyword_ids = tokenizer(keyword).input_ids
            if len(cur_keyword_ids) > 1 and cur_keyword_ids[0] == tokenizer.bos_token_id:
                cur_keyword_ids = cur_keyword_ids[1:]
            if len(cur_keyword_ids) > self.max_keyword_len:
                self.max_keyword_len = len(cur_keyword_ids)
            self.keyword_ids.append(torch.tensor(cur_keyword_ids))
        self.tokenizer = tokenizer
        self.start_len = input_ids.shape[1]
    
    def call_for_batch(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        offset = min(output_ids.shape[1] - self.start_len, self.max_keyword_len)
        self.keyword_ids = [keyword_id.to(output_ids.device) for keyword_id in self.keyword_ids]
        for keyword_id in self.keyword_ids:
            truncated_output_ids = output_ids[0, -keyword_id.shape[0]:]
            if torch.equal(truncated_output_ids, keyword_id):
                return True
        outputs = self.tokenizer.batch_decode(output_ids[:, -offset:], skip_special_tokens=True)[0]
        for keyword in self.keywords:
            if keyword in outputs:
                return True
        return False
    
    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        outputs = []
        for i in range(output_ids.shape[0]):
            outputs.append(self.call_for_batch(output_ids[i].unsqueeze(0), scores))
        return all(outputs)

</file>
<file : llava/model/multimodal_encoder/clip_encoder.py>
import warnings
import torch
import torch.nn as nn
import os

from safetensors.torch import load_file as load_safetensors
from INFERclipregXGATED.model import build_model as build_reg_gated_clip
from INFERclipregXGATED.clip import _transform as reg_gated_transform

from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig


class CLIPVisionTower(nn.Module):
    def __init__(self, vision_tower, args, delay_load=False):
        super().__init__()

        self.is_loaded = False
        self.args = args # LlavaConfig 저장 (이것이 __init__에 전달된다고 가정)

        self.vision_tower_name = vision_tower

        self._num_registers = 0 # GATED 모델이 아니면 0, 또는 load_model에서 설정될 때까지 임시값
        self._actual_patch_size = 14 # 기본값, load_model에서 실제 값으로 업데이트
        self._input_resolution = 224 # 기본값, load_model에서 실제 값으로 업데이트

        self.select_layer = args.mm_vision_select_layer
        self.select_feature = getattr(args, 'mm_vision_select_feature', 'patch')
        self._device = None

        if not delay_load:
            self.load_model()
        elif getattr(args, 'unfreeze_mm_vision_tower', False):
            self.load_model()

        # CUSTOM: "GATED" 로컬 모델이고 delay_load=True 인 경우,
        # CLIPVisionConfig.from_pretrained() 호출을 피하고,
        # 나중에 load_model()에서 실제 모델 로드 후 config를 설정합니다.
        elif "GATED" in self.vision_tower_name.upper():
            # 이 경우, cfg_only는 None으로 두거나, 최소한의 기본값으로 설정할 수 있습니다.
            # self.hidden_size 같은 속성은 load_model()이 호출된 후에야 정확해집니다.
            self.cfg_only = None # 또는 기본 CLIPVisionConfig() 인스턴스

        else:
            self.cfg_only = CLIPVisionConfig.from_pretrained(self.vision_tower_name)

    def load_model(self, device_map=None):
        if self.is_loaded:
            print(f'{self.vision_tower_name} is already loaded, skipping.')
            return

        # "GATED" 모델 이름 감지 → Reg-Gated CLIP 로드 분기
        if isinstance(self.vision_tower_name, str) and "GATED" in self.vision_tower_name.upper():
            # 1) checkpoint 읽어 build_model 호출

            vision_tower_path = self.vision_tower_name
            # model_dir_for_vision_tower는 LlavaConfig (self.args)에 저장되어 있다고 가정
            if not os.path.isabs(vision_tower_path) and hasattr(self.args, 'model_dir_for_vision_tower') and self.args.model_dir_for_vision_tower is not None:
                vision_tower_path = os.path.join(self.args.model_dir_for_vision_tower, self.vision_tower_name)
            
            if not os.path.exists(vision_tower_path):
                raise FileNotFoundError(f"Custom vision tower GATED file not found at {vision_tower_path}. Original name: {self.vision_tower_name}, Model dir: {getattr(self.args, 'model_dir_for_vision_tower', 'Not Set')}")
            state = load_safetensors(self.vision_tower_name)


            # 2) visual encoder만 뽑아서 dtype/device 설정
            target_dtype = self.dtype
            # build_reg_gated_clip 호출 시 target_dtype 전달
            full_clip = build_reg_gated_clip(state, model_dtype=target_dtype)

            # device_map 처리: "auto"인 경우 실제 장치로 변환
            resolved_device = device_map
            if device_map == "auto":
                # "auto"는 여기서 직접 사용할 수 없으므로, 주 장치를 선택하거나 CPU를 사용
                resolved_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            elif device_map is None: # device_map이 명시적으로 None이면 기본 CUDA 사용
                resolved_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

            self._device = resolved_device # 결정된 device를 인스턴스 변수에 저장!

            self.vision_tower = full_clip.visual.to(device=self._device, dtype=target_dtype)
            self.vision_tower.requires_grad_(False)

            # 3) 등록 토큰 수, 해상도, 패치크기 얻기
            self._num_registers = getattr(self.vision_tower, 'num_registers', 4) # GATED 모델의 num_registers 속성 사용
            self._input_resolution = getattr(self.vision_tower, 'input_resolution', 224)
            self._actual_patch_size = getattr(self.vision_tower, 'patch_size', 14)

            # 모델 로드 후, config 객체를 실제 모델의 config로 설정
            # VisionTransformer에 self.width가 직접 없으므로, conv1.out_channels 등에서 가져옴
            # 또는 class_embedding.shape[-1], positional_embedding.shape[-1] 등도 가능
            vision_tower_width = self.vision_tower.conv1.out_channels
            self.vision_tower.config = self._build_dummy_config_for_gated(
                vision_tower_width,
                self._input_resolution,
                self._actual_patch_size
                )

            # 4) 전처리기 설정
            self.image_processor = reg_gated_transform(self._input_resolution)

            # CUSTOM: Compose 객체에 image_mean, image_std 속성 추가
            # 이 값들은 reg_gated_transform 내부의 Normalize에 사용된 값이어야 함.
            # 일반적으로 CLIP 표준 값을 사용하거나, 모델 학습 시 사용된 특정 값을 사용.
            # 예시로 CLIP 표준 값을 사용. 실제 값으로 변경 필요.
            setattr(self.image_processor, 'image_mean', getattr(self.args, 'custom_image_mean', [0.48145466, 0.4578275, 0.40821073]))
            setattr(self.image_processor, 'image_std', getattr(self.args, 'custom_image_std', [0.26862954, 0.26130258, 0.27577711]))
            setattr(self.image_processor, 'crop_size', {'height': self._input_resolution, 'width': self._input_resolution})
            setattr(self.image_processor, 'size', {'shortest_edge': self._input_resolution})

            self.is_loaded = True
            return

        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)
        # 표준 모델 로드 시에도 self.dtype을 존중하도록 torch_dtype 인자 전달
        self.vision_tower = CLIPVisionModel.from_pretrained(
            self.vision_tower_name,
            device_map=device_map,
            torch_dtype=target_dtype)
        self.vision_tower.requires_grad_(False)

        # HF 모델의 경우, 첫 번째 파라미터의 device를 self._device로 설정
        if len(list(self.vision_tower.parameters())) > 0:
            self._device = next(self.vision_tower.parameters()).device
        elif device_map == "auto": # 파라미터가 없지만 auto인 경우 (매우 드문 케이스)
                self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        elif isinstance(device_map, str): # "cuda:0" 등 명시적 장치 문자열
                self._device = torch.device(device_map)
        else: # 기타 (None 등)
                self._device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.is_loaded = True

    def _build_dummy_config_for_gated(self, hidden_size, image_size, patch_size):
        """GATED 모델 로드 후, 호환성을 위해 최소한의 config 객체를 생성합니다."""
        # CLIPVisionConfig의 필수 필드들을 채워줍니다.
        # 실제 CLIPVisionConfig 인스턴스를 생성하고 값을 할당하는 것이 좋습니다.
        config = CLIPVisionConfig(
            hidden_size=hidden_size,
            image_size=image_size,
            patch_size=patch_size,
            # projection_dim 등 다른 필요한 기본값들을 추가할 수 있습니다.
        )
        return config

    def feature_select(self, image_forward_outs):
        image_features = image_forward_outs.hidden_states[self.select_layer]
        if self.select_feature == 'patch':
            image_features = image_features[:, 1:]
        elif self.select_feature == 'cls_patch':
            image_features = image_features
        else:
            raise ValueError(f'Unexpected select feature: {self.select_feature}')
        return image_features

    @torch.no_grad()
    def forward(self, images):
        if type(images) is list:
            image_features = []
            for image in images:
                image_forward_out = self.vision_tower(image.to(device=self.device, dtype=self.dtype).unsqueeze(0), output_hidden_states=True)
                image_feature = self.feature_select(image_forward_out).to(image.dtype)
                image_features.append(image_feature)
        else:
            image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)
            image_features = self.feature_select(image_forward_outs).to(images.dtype)

        return image_features

    @property
    def dummy_feature(self):
        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)

    # @property
    # def dtype(self):
    #     if self.vision_tower is not None and hasattr(self.vision_tower, 'dtype'): # 실제 모델의 dtype 우선
    #         return self.vision_tower.dtype
    #     # self.args (LlavaConfig) 기반 dtype 결정 로직 (이전과 동일)
    #     if hasattr(self.args, 'torch_dtype') and self.args.torch_dtype is not None:
    #         return self.args.torch_dtype
    #     elif getattr(self.args, 'fp16', False):
    #         return torch.float16
    #     elif getattr(self.args, 'bf16', False):
    #         return torch.bfloat16
    #     return torch.float32


    @property
    def dtype(self) -> torch.dtype:
        # 1. 모델이 이미 로드되었고, 실제 vision_tower 모듈에 dtype 속성이 있다면 그것을 사용
        if self.is_loaded and self.vision_tower is not None and hasattr(self.vision_tower, 'dtype'):
            return self.vision_tower.dtype
        
        # 2. self.args (LlavaConfig 인스턴스)가 설정되어 있는지 확인
        if not hasattr(self, 'args') or self.args is None:
            warnings.warn(
                "CLIPVisionTower.args (LlavaConfig) is not set. "
                "Defaulting dtype to torch.bfloat16 based on user preference. "
                "This might be unintended if args were expected to provide dtype info."
            )
            return torch.bfloat16 # 사용자가 bf16을 원하므로, 비상시 기본값으로 bf16

        # 3. LlavaConfig에 저장된 torch_dtype 객체를 직접 사용 (가장 우선)
        #    LlavaConfig.__init__에서 self.torch_dtype에 실제 torch.dtype 객체를 저장했다고 가정
        if hasattr(self.args, 'torch_dtype') and isinstance(self.args.torch_dtype, torch.dtype):
            return self.args.torch_dtype
        
        # 4. LlavaConfig에 torch_dtype이 문자열로 저장된 경우 처리 (예: "bfloat16")
        #    (LlavaConfig.__init__에서 self.torch_dtype에 문자열을 저장했을 경우)
        if hasattr(self.args, 'torch_dtype') and isinstance(self.args.torch_dtype, str):
            dtype_str = self.args.torch_dtype.lower()
            if dtype_str == "bfloat16":
                return torch.bfloat16
            elif dtype_str == "float16":
                return torch.float16
            elif dtype_str == "float32":
                return torch.float32
            else:
                warnings.warn(f"Unsupported torch_dtype string '{self.args.torch_dtype}' in LlavaConfig. Defaulting to bfloat16.")
                return torch.bfloat16 # 알 수 없는 문자열이면 bf16으로

        # 5. LlavaConfig의 bf16 또는 fp16 플래그를 확인 (torch_dtype 속성이 없는 경우의 fallback)
        #    bf16을 우선적으로 확인
        if getattr(self.args, 'bf16', False):
            return torch.bfloat16
        elif getattr(self.args, 'fp16', False):
            return torch.float16
        
        # 6. 모든 조건에 해당하지 않으면, 사용자가 bf16을 원한다고 했으므로 bfloat16 반환
        #    또는 LlamaConfig의 기본 dtype을 따르도록 torch.float32를 반환할 수도 있음
        #    여기서는 사용자 요청에 따라 bf16
        return torch.bfloat16

    @property
    def num_registers(self):
        # load_model에서 설정된 _num_registers 값을 반환
        # 또는 GATED 모델의 경우 여기서 하드코딩된 값을 반환할 수도 있음
        if "GATED" in self.vision_tower_name.upper():
            return self._num_registers # load_model에서 설정된 값을 따름 (또는 여기서 4로 고정해도 됨)
        # 표준 HF CLIP 모델은 num_registers가 없으므로 0 반환
        return 0

    @property
    def device(self):
        if self._device is not None:
            return self._device
        # fallback (load_model이 호출되지 않은 극단적인 경우)
        return torch.device("cuda" if torch.cuda.is_available() else "cpu")

    @property
    def config(self):
        if self.is_loaded:
            return self.vision_tower.config
        else:
            return self.cfg_only

    @property
    def hidden_size(self):
        return self.config.hidden_size

    @property
    def num_patches_per_side(self):
        """
        Calculates the number of patches on one side of the square image.
        This value depends on whether the model is loaded, and if it's a GATED model
        or a standard Hugging Face CLIP model.
        """
        if self.is_loaded: # 모델이 로드된 후
            if "GATED" in self.vision_tower_name.upper():
                # GATED 모델: load_model에서 설정된 내부 변수 사용
                # self._input_resolution과 self._actual_patch_size는 load_model에서 설정됨
                if self._actual_patch_size == 0: # 패치 크기가 0이면 나눗셈 오류 방지
                    return 0 
                return self._input_resolution // self._actual_patch_size
            else: # 표준 HF CLIP 모델: 로드된 모델의 config 사용
                if self.vision_tower is not None and hasattr(self.vision_tower, 'config'):
                    config = self.vision_tower.config
                    if config.patch_size == 0: return 0
                    return config.image_size // config.patch_size
                else: # 로드는 되었으나 config 접근 불가 (예외적 상황)
                    warnings.warn("Vision tower loaded but config not accessible, falling back to LlavaConfig for patch info.")
                    # LlavaConfig의 기본값이나 사용자 설정값으로 fallback
                    image_size = getattr(self.args, 'vision_image_size', 224) # LlavaConfig에 vision_image_size 추가 필요
                    patch_size = getattr(self.args, 'vision_patch_size', 14) # LlavaConfig에 vision_patch_size 추가 필요
                    if patch_size == 0: return 0
                    return image_size // patch_size
        else: # 모델이 로드되기 전 (delay_load=True 인 경우)
            if "GATED" in self.vision_tower_name.upper():
                # GATED 모델 로드 전: LlavaConfig의 기본값 또는 사용자 설정값 사용
                # (이 값들은 GATED 모델의 실제 값과 일치해야 함)
                image_size = getattr(self.args, 'vision_image_size', 224) # LlavaConfig에 vision_image_size 필드 추가 고려
                patch_size = getattr(self.args, 'vision_patch_size', 14) # LlavaConfig에 vision_patch_size 필드 추가 고려
                if patch_size == 0: return 0
                return image_size // patch_size
            elif self.cfg_only is not None: # 표준 HF CLIP 모델 로드 전 (cfg_only 사용)
                if self.cfg_only.patch_size == 0: return 0
                return self.cfg_only.image_size // self.cfg_only.patch_size
            else: # cfg_only도 없는 경우 (예: GATED 모델이 아닌데 경로가 잘못된 경우 등)
                warnings.warn("Cannot determine num_patches_per_side before model loading without cfg_only or GATED model hints.")
                return 0 # 또는 적절한 기본값 (예: 224 // 14 = 16)

    @property
    def num_patches(self):
        """
        Calculates the total number of patches.
        """
        n_patches_side = self.num_patches_per_side
        return n_patches_side * n_patches_side



class CLIPVisionTowerS2(CLIPVisionTower):
    def __init__(self, vision_tower, args, delay_load=False):
        super().__init__(vision_tower, args, delay_load)

        self.s2_scales = getattr(args, 's2_scales', '336,672,1008')
        self.s2_scales = list(map(int, self.s2_scales.split(',')))
        self.s2_scales.sort()
        self.s2_split_size = self.s2_scales[0]
        self.s2_image_size = self.s2_scales[-1]

        try:
            from s2wrapper import forward as multiscale_forward
        except ImportError:
            raise ImportError('Package s2wrapper not found! Please install by running: \npip install git+https://github.com/bfshi/scaling_on_scales.git')
        self.multiscale_forward = multiscale_forward

        # change resize/crop size in preprocessing to the largest image size in s2_scale
        if not delay_load or getattr(args, 'unfreeze_mm_vision_tower', False):
            self.image_processor.size['shortest_edge'] = self.s2_image_size
            self.image_processor.crop_size['height'] = self.image_processor.crop_size['width'] = self.s2_image_size

    def load_model(self, device_map=None):
        if self.is_loaded:
            print('{} is already loaded, `load_model` called again, skipping.'.format(self.vision_tower_name))
            return

        self.image_processor = CLIPImageProcessor.from_pretrained(self.vision_tower_name)
        self.vision_tower = CLIPVisionModel.from_pretrained(self.vision_tower_name, device_map=device_map)
        self.vision_tower.requires_grad_(False)

        self.image_processor.size['shortest_edge'] = self.s2_image_size
        self.image_processor.crop_size['height'] = self.image_processor.crop_size['width'] = self.s2_image_size

        self.is_loaded = True

    @torch.no_grad()
    def forward_feature(self, images):
        image_forward_outs = self.vision_tower(images.to(device=self.device, dtype=self.dtype), output_hidden_states=True)
        image_features = self.feature_select(image_forward_outs).to(images.dtype)
        return image_features

    @torch.no_grad()
    def forward(self, images):
        if type(images) is list:
            image_features = []
            for image in images:
                image_feature = self.multiscale_forward(self.forward_feature, image.unsqueeze(0), img_sizes=self.s2_scales, max_split_size=self.s2_split_size)
                image_features.append(image_feature)
        else:
            image_features = self.multiscale_forward(self.forward_feature, images, img_sizes=self.s2_scales, max_split_size=self.s2_split_size)

        return image_features

    @property
    def hidden_size(self):
        return self.config.hidden_size * len(self.s2_scales)
</file>
- 수정할 부분은 블럭단위 혹은 함수단위 중 작은 것을 골라 전문을 생략없이 제공하는 것을 원칙으로 한다. 
             - 특정 코드를 수정할 때에는 다른 코드의 삭제 또는 수정을 최소화하도록 한다. 
             - 눈 앞에 급급하게 보이는 현상만을 처리하려 하지 말고, 근본적인 이유를 고민해본다. 
