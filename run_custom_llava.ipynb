{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82ee4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/regllava/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-13 08:52:36,363] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Added to sys.path: /home/ubuntu/Projects/regllava\n",
      "✔ CustomVisionTransformer imported.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # LLaVA Custom Vision Encoder / Projector Inference Notebook\n",
    "# 사용자 정의 Reg‑Gated ViT‑L/14 + 맞춤 Projector 를 Vicuna‑7B 기반 LLaVA 모델에 적용하여\n",
    "# – 모델 로드, 이미지 추론, Cross‑Attention 시각화 – 를 수행합니다.\n",
    "\n",
    "# %% ---------------------------------------------------------------------------\n",
    "# 0. 환경 설정 & 공통 라이브러리\n",
    "# ---------------------------------------------------------------------------\n",
    "import os, sys, warnings, shutil\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-12.4\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "    \"/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.4/lib64:\"\n",
    "    + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    ")\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import numpy as np, cv2, math\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, BitsAndBytesConfig, TextStreamer\n",
    ")\n",
    "from llava.constants import (\n",
    "    IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN,\n",
    "    DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    ")\n",
    "from llava.conversation import conv_templates, SeparatorStyle\n",
    "from llava.utils import disable_torch_init\n",
    "from llava.mm_utils import (\n",
    "    process_images, tokenizer_image_token, get_model_name_from_path\n",
    ")\n",
    "from llava.model.builder import load_pretrained_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# LLaVA 프로젝트 루트가 sys.path 에 없으면 추가\n",
    "nb_root = os.getcwd()\n",
    "if nb_root not in sys.path:\n",
    "    sys.path.insert(0, nb_root)\n",
    "    print(\"Added to sys.path:\", nb_root)\n",
    "\n",
    "# 사용자 정의 CLIP\n",
    "from INFERclipregXGATED.model import VisionTransformer as CustomVisionTransformer\n",
    "print(\"✔ CustomVisionTransformer imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c6d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 1. 사용자 설정\n",
    "# ---------------------------------------------------------------------------\n",
    "MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR = \"./llava-v1.5-7b-local\"\n",
    "MODEL_BASE_LLM = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "CUSTOM_VISION_ENCODER_WEIGHTS_PATH = \"./models/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors\"\n",
    "CUSTOM_PROJECTOR_FILENAME = \"mm_projector_2epoch.bin\"\n",
    "\n",
    "IMAGE_FILE_TO_PROCESS = \"data/car.jpg\"\n",
    "USER_PROMPT = \"Describe the where the main object is located in the image.\"\n",
    "\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOAD_8BIT = False\n",
    "LOAD_4BIT = False\n",
    "\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE = 0.2\n",
    "CONV_MODE = None          # \"vicuna_v1\" 등으로 지정 가능, None 이면 자동 추론\n",
    "\n",
    "VISION_ENCODER_CONFIG = dict(\n",
    "    image_resolution=224, patch_size=14,\n",
    "    width=1024, layers=24, heads=16,\n",
    "    output_dim=1024, num_registers=4\n",
    ")\n",
    "mm_vision_select_layer_val = -2\n",
    "mm_projector_type_val      = \"mlp2x_gelu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c35e2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 2. 유틸리티 함수\n",
    "# ---------------------------------------------------------------------------\n",
    "def load_image(path_or_url: str) -> Image.Image:\n",
    "    if path_or_url.startswith((\"http://\", \"https://\")):\n",
    "        resp = requests.get(path_or_url); resp.raise_for_status()\n",
    "        return Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "    if not os.path.exists(path_or_url):\n",
    "        raise FileNotFoundError(path_or_url)\n",
    "    return Image.open(path_or_url).convert(\"RGB\")\n",
    "\n",
    "def overlay_heatmap(hm: np.ndarray, base: Image.Image) -> np.ndarray:\n",
    "    bgr = cv2.cvtColor(np.array(base), cv2.COLOR_RGB2BGR)\n",
    "    h, w = bgr.shape[:2]\n",
    "    hm = cv2.resize(hm.astype(np.float32), (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "    hm = (hm - hm.min()) / (hm.ptp() + 1e-8)\n",
    "    hm_cm = cv2.applyColorMap((hm * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    mix  = cv2.addWeighted(bgr, 0.6, hm_cm, 0.4, 0)\n",
    "    return cv2.cvtColor(mix, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def infer_conv_mode(model_name: str) -> str:\n",
    "    name = model_name.lower()\n",
    "    if \"llama-2\"   in name: return \"llava_llama_2\"\n",
    "    if \"mistral\"   in name: return \"mistral_instruct\"\n",
    "    if \"v1.6-34b\"  in name: return \"chatml_direct\"\n",
    "    if \"v1\"        in name: return \"llava_v1\"\n",
    "    if \"mpt\"       in name: return \"mpt\"\n",
    "    return \"llava_v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f74655c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Loading model...\n",
      "Attempting to load model with 8-bit quantization (LOAD_8BIT=True)\n",
      "Loading LLaVA from base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 17225.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Projects/regllava/INFERclipregXGATED 에서 INFERclipregXGATED 모듈을 성공적으로 임포트했습니다.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Parameter' object has no attribute 'SCB'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m     13\u001b[39m model_arch_name = get_model_name_from_path(MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m tokenizer, model, image_processor, _ctx_len = \u001b[43mload_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_BASE_LLM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_arch_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOAD_8BIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 8비트 로드 활성화\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOAD_4BIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# bitsandbytes는 device_map=\"auto\"를 선호하지만, 단일 GPU 유지를 위해 그대로 둠\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 단일 GPU에 고정\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meager\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 어텐션 구현 명시 (선택적)\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# torch_dtype 인자는 양자화 시 내부적으로 처리되므로 제거하거나 torch.float16 유지 가능\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Model loaded (LLM part potentially quantized).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# → MAIN_DEV 정의 (모델 파라미터가 올라간 디바이스)\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# 양자화 모델은 device 속성이 명확하지 않을 수 있으므로, GPU가 사용 가능한지 확인하는 방식으로 변경\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/regllava/llava/model/builder.py:147\u001b[39m, in \u001b[36mload_pretrained_model\u001b[39m\u001b[34m(model_path, model_base, model_name, load_8bit, load_4bit, device_map, device, use_flash_attn, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m cfg = AutoConfig.from_pretrained(model_path)\n\u001b[32m    146\u001b[39m _sanitize_llava_cfg(cfg)                    \u001b[38;5;66;03m# ← 강화된 함수 호출\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m model = \u001b[43mLlavaLlamaForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# --- projector 가중치 로드 (원본 그대로) ---\u001b[39;00m\n\u001b[32m    151\u001b[39m mm_proj = torch.load(os.path.join(model_path, \u001b[33m\"\u001b[39m\u001b[33mmm_projector.bin\u001b[39m\u001b[33m\"\u001b[39m), map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/transformers/modeling_utils.py:4399\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4389\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4390\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4392\u001b[39m     (\n\u001b[32m   4393\u001b[39m         model,\n\u001b[32m   4394\u001b[39m         missing_keys,\n\u001b[32m   4395\u001b[39m         unexpected_keys,\n\u001b[32m   4396\u001b[39m         mismatched_keys,\n\u001b[32m   4397\u001b[39m         offload_index,\n\u001b[32m   4398\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4399\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4405\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4408\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4409\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4415\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4417\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4418\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/transformers/modeling_utils.py:4689\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   4686\u001b[39m model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n\u001b[32m   4688\u001b[39m \u001b[38;5;66;03m# correctly initialize the missing (and potentially mismatched) keys\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4689\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_initialize_missing_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4691\u001b[39m \u001b[38;5;66;03m# Set some modules to fp32 if needed\u001b[39;00m\n\u001b[32m   4692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_in_fp32_regex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/transformers/modeling_utils.py:5240\u001b[39m, in \u001b[36mPreTrainedModel._initialize_missing_keys\u001b[39m\u001b[34m(self, loaded_keys, ignore_mismatched_sizes, is_quantized)\u001b[39m\n\u001b[32m   5234\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\u001b[39;00m\n\u001b[32m   5235\u001b[39m \u001b[33;03m`_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\u001b[39;00m\n\u001b[32m   5236\u001b[39m \u001b[33;03mbe initialized correctly (i.e. weight initialization distribution).\u001b[39;00m\n\u001b[32m   5237\u001b[39m \u001b[33;03mAlso take care of setting the `_is_hf_initialized` flag for keys that are not missing.\u001b[39;00m\n\u001b[32m   5238\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ignore_mismatched_sizes:\n\u001b[32m-> \u001b[39m\u001b[32m5240\u001b[39m     not_initialized_submodules = \u001b[43mset_initialized_submodules\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5241\u001b[39m     \u001b[38;5;66;03m# If we're about to tie the output embeds to the input embeds we don't need to init them\u001b[39;00m\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   5243\u001b[39m         \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.get_text_config(decoder=\u001b[38;5;28;01mTrue\u001b[39;00m), \u001b[33m\"\u001b[39m\u001b[33mtie_word_embeddings\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5244\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.get_text_config(decoder=\u001b[38;5;28;01mTrue\u001b[39;00m).tie_word_embeddings\n\u001b[32m   5245\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/transformers/modeling_utils.py:594\u001b[39m, in \u001b[36mset_initialized_submodules\u001b[39m\u001b[34m(model, state_dict_keys)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module_name, module \u001b[38;5;129;01min\u001b[39;00m model.named_modules():\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_name == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    593\u001b[39m         \u001b[38;5;66;03m# When checking if the root module is loaded there's no need to prepend module_name.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m         module_keys = \u001b[38;5;28mset\u001b[39m(\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    595\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    596\u001b[39m         module_keys = {\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m module.state_dict()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1895\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   1893\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   1894\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1895\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1896\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   1897\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1895\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   1893\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   1894\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1895\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1896\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   1897\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1895\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   1893\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   1894\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1895\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1896\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   1897\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/torch/nn/modules/module.py:1892\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_pre_hooks.values():\n\u001b[32m   1891\u001b[39m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[32m-> \u001b[39m\u001b[32m1892\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_to_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1893\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   1894\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:919\u001b[39m, in \u001b[36mLinear8bitLt._save_to_state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars)\u001b[39m\n\u001b[32m    916\u001b[39m scb_name = \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;66;03m# case 1: .cuda was called, SCB is in self.weight\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m param_from_weight = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.weight, scb_name)\n\u001b[32m    920\u001b[39m \u001b[38;5;66;03m# case 2: self.init_8bit_state was called, SCB is in self.state\u001b[39;00m\n\u001b[32m    921\u001b[39m param_from_state = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.state, scb_name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Parameter' object has no attribute 'SCB'"
     ]
    }
   ],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 3. 모델 로드 (builder.py 수정 버전 사용)\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"▶ Loading model...\")\n",
    "disable_torch_init()\n",
    "\n",
    "model_arch_name = get_model_name_from_path(MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR)\n",
    "tokenizer, model, image_processor, _ctx_len = load_pretrained_model(\n",
    "    MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR,\n",
    "    model_base=MODEL_BASE_LLM,\n",
    "    model_name=model_arch_name,\n",
    "    load_8bit=LOAD_8BIT, load_4bit=LOAD_4BIT,\n",
    "    device=DEVICE,\n",
    "    device_map={ \"\": DEVICE },          # ← 모든 서브모듈을 ***단일 GPU*** 로 고정\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "print(\"✅ Model loaded.\")\n",
    "\n",
    "# → MAIN_DEV 정의 (모델 파라미터가 올라간 디바이스)\n",
    "MAIN_DEV = next(model.parameters()).device\n",
    "\n",
    "\n",
    "# ─── Method B: pad_token을 eos와 구분하도록 special_tokens 추가 ─────────────────\n",
    "if tokenizer.pad_token_id == tokenizer.eos_token_id:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3‑1. 사용할 **단일 GPU** 확정 & 모듈·입력 tensor 통일\n",
    "# ---------------------------------------------------------------------------\n",
    "#  • Accelerate `device_map=\"auto\"` 로드 시 모듈이 GPU‑0, GPU‑1 에 분산될 수 있음\n",
    "#  • Vision Tower 출력 → mm_projector → LLM 으로 흐르므로\n",
    "#    세 모듈·입력이 **한 GPU** 에 있어야 dtype/device 오류가 안 난다\n",
    "\n",
    "MAIN_DEV = next(model.parameters()).device          # 보통 cuda:0\n",
    "\n",
    "proj_path = os.path.join(MODEL_PATH_LLAVA_CONFIG_AND_PROJECTOR,\n",
    "                         CUSTOM_PROJECTOR_FILENAME)\n",
    "assert os.path.exists(proj_path), f\"Projector file not found: {proj_path}\"\n",
    "raw_state = torch.load(proj_path, map_location=\"cpu\")   # CPU 로 우선 읽기\n",
    "\n",
    "def extract_mm_projector(sd):\n",
    "    cleaned = {}\n",
    "    for k, v in sd.items():\n",
    "        if \"mm_projector\" not in k:\n",
    "            continue                      # 다른 서브모듈은 무시\n",
    "        k = k.split(\"mm_projector.\", 1)[-1]   # → 0.weight …\n",
    "        if k.startswith(\"module.\"):\n",
    "            k = k[7:]                     # DataParallel prefix 제거\n",
    "        cleaned[k] = v\n",
    "    return cleaned\n",
    "\n",
    "proj_state = extract_mm_projector(raw_state)\n",
    "if not proj_state:\n",
    "    raise ValueError(\"mm_projector keys가 체크포인트에 없습니다!\")\n",
    "\n",
    "# ② GPU · dtype 맞춰서 로드\n",
    "model.get_model().mm_projector.to(device=MAIN_DEV, dtype=model.dtype)\n",
    "model.get_model().mm_projector.load_state_dict(proj_state, strict=True)\n",
    "print(\"✅  mm_projector weights loaded:\", len(proj_state), \"tensors\")\n",
    "\n",
    "# ① Vision Tower\n",
    "vt = model.get_vision_tower()\n",
    "# —— 1) Reg-Gated branch 강제 활성화 —— \n",
    "setattr(vt, \"is_custom_reg_gated_clip\", True)\n",
    "# —— 2) config 에 우리가 원하는 레이어·프로젝터 세팅 심기 —— \n",
    "model.config.mm_vision_select_layer = mm_vision_select_layer_val\n",
    "model.config.mm_projector_type      = mm_projector_type_val\n",
    "\n",
    "# ==== 디버깅: CustomVisionTransformer 생성자 시그니처 확인 ====\n",
    "import inspect\n",
    "sig = inspect.signature(CustomVisionTransformer.__init__)\n",
    "print(\"Debug: CustomVisionTransformer.__init__ signature:\", sig)\n",
    "print(CustomVisionTransformer.__init__.__doc__)\n",
    "# ==== 디버깅 종료 ====\n",
    "\n",
    "# —— 커스텀 Reg-Gated ViT-L/14 인코더로 교체 & 가중치 로드 ——\n",
    "from safetensors.torch import load_file as load_safetensors\n",
    "# VISION_ENCODER_CONFIG 키 이름을 생성자 시그니처(input_resolution, patch_size, width, layers, heads, output_dim, num_registers) 에 맞춰 매핑\n",
    "custom_vt = CustomVisionTransformer(\n",
    "    input_resolution = VISION_ENCODER_CONFIG['image_resolution'],\n",
    "    patch_size       = VISION_ENCODER_CONFIG['patch_size'],\n",
    "    width            = VISION_ENCODER_CONFIG['width'],\n",
    "    layers           = VISION_ENCODER_CONFIG['layers'],\n",
    "    heads            = VISION_ENCODER_CONFIG['heads'],\n",
    "    output_dim       = VISION_ENCODER_CONFIG['output_dim'],\n",
    "    num_registers    = VISION_ENCODER_CONFIG['num_registers'],\n",
    ")\n",
    "\n",
    "# ——— checkpoint 읽어서 모델에 맞게 필터링 후 로드 ———\n",
    "state_v = load_safetensors(CUSTOM_VISION_ENCODER_WEIGHTS_PATH)\n",
    "from collections import OrderedDict\n",
    "filtered_state = OrderedDict()\n",
    "# checkpoint key 에 'visual.' prefix 있으면 제거\n",
    "for k, v in state_v.items():\n",
    "    name = k[len(\"visual.\"):] if k.startswith(\"visual.\") else k\n",
    "    # 모델에 존재하고 모양이 정확히 같은 파라미터만 담기\n",
    "    if name in custom_vt.state_dict() and custom_vt.state_dict()[name].shape == v.shape:\n",
    "        filtered_state[name] = v\n",
    "# strict=False 로 불일치 항목(크기/이름) 무시하고 로드\n",
    "missing_keys, unexpected_keys = custom_vt.load_state_dict(filtered_state, strict=False)\n",
    "print(f\"✅ Custom vision encoder loaded: {len(filtered_state)} tensors\")\n",
    "if missing_keys:\n",
    "    print(\"   → missing keys:\", missing_keys)\n",
    "if unexpected_keys:\n",
    "    print(\"   → unexpected keys (ignored):\", unexpected_keys)\n",
    "\n",
    "custom_vt.to(device=MAIN_DEV, dtype=model.dtype)\n",
    "# LLaVA wrapper 안의 vision_tower 속성에 덮어쓰기\n",
    "vt.vision_tower = custom_vt\n",
    "print(\"✅ Custom vision encoder loaded:\", CUSTOM_VISION_ENCODER_WEIGHTS_PATH)\n",
    "\n",
    "# 이후 기존 방식대로 single-GPU, dtype 통일\n",
    "vt.to(device=MAIN_DEV, dtype=model.dtype)\n",
    "if hasattr(vt, \"vision_tower\"):\n",
    "    vt.vision_tower.to(device=MAIN_DEV, dtype=model.dtype)\n",
    "\n",
    "# ② Projector\n",
    "model.get_model().mm_projector.to(device=MAIN_DEV, dtype=model.dtype)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# conv 템플릿 선택\n",
    "conv_key = CONV_MODE or infer_conv_mode(model_arch_name)\n",
    "if conv_key not in conv_templates:\n",
    "    raise ValueError(f\"Unknown conversation mode '{conv_key}'\")\n",
    "conv = conv_templates[conv_key].copy()\n",
    "roles = (\"user\", \"assistant\") if \"mpt\" in model_arch_name.lower() else conv.roles\n",
    "print(\"Conversation mode →\", conv_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a77aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 4. 이미지 로드 & 전처리\n",
    "# ---------------------------------------------------------------------------\n",
    "# --- HF 모델 출력 타입 사용 위해 import 추가 ---\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPooling\n",
    "import torch # torch import 확인\n",
    "import torch.nn as nn # nn import 확인\n",
    "# -------------------------------------------\n",
    "pil_img = load_image(IMAGE_FILE_TO_PROCESS)\n",
    "img_size = pil_img.size\n",
    "print(\"Loaded image:\", img_size)\n",
    "\n",
    "img_tensor = process_images([pil_img], image_processor, model.config)\n",
    "# LLaVA forward 는 list[Tensor] 를 요구 ⇒ 항상 list 로 맞춘다\n",
    "if not isinstance(img_tensor, list):\n",
    "    img_tensor = [img_tensor]\n",
    "# --- 초기 dtype 결정은 모델 로드 시 설정된 model.dtype을 따르도록 함 ---\n",
    "# img_tensor = [t.to(MAIN_DEV, dtype=torch.float32) for t in img_tensor] # 여기서 강제 float32 변환 제거\n",
    "img_tensor = [t.to(MAIN_DEV, dtype=model.dtype) for t in img_tensor]\n",
    "print(f\"Debug: Initial img_tensor dtype set to model.dtype: {model.dtype}\")\n",
    "\n",
    "\n",
    "# ==== 디버깅 코드 추가 시작 ====\n",
    "# Vision tower 타입 확인\n",
    "print(\"Debug: Vision tower type:\", type(model.get_vision_tower()))\n",
    "# --- 모델 및 주요 모듈의 현재 dtype 확인 ---\n",
    "print(f\"Debug: Initial model.dtype: {model.dtype}\")\n",
    "# vision_tower 객체가 None이 아니고, 파라미터가 있을 경우 첫 파라미터의 dtype 확인\n",
    "vt_initial_dtype = \"N/A\"\n",
    "if model.get_vision_tower() is not None and hasattr(model.get_vision_tower(), 'dtype'):\n",
    "    vt_initial_dtype = model.get_vision_tower().dtype\n",
    "elif model.get_vision_tower() is not None:\n",
    "     try:\n",
    "          vt_initial_dtype = next(model.get_vision_tower().parameters()).dtype\n",
    "     except StopIteration:\n",
    "          vt_initial_dtype = \"No Params\"\n",
    "print(f\"Debug: Initial vision_tower.dtype: {vt_initial_dtype}\")\n",
    "# mm_projector의 첫 레이어 가중치 dtype 확인 (존재 및 타입 체크 추가)\n",
    "proj_initial_dtype = \"N/A\"\n",
    "if hasattr(model.get_model(), 'mm_projector') and model.get_model().mm_projector is not None:\n",
    "    projector_module = model.get_model().mm_projector\n",
    "    if isinstance(projector_module, nn.Sequential) and len(projector_module) > 0 and hasattr(projector_module[0], 'weight'):\n",
    "        proj_initial_dtype = projector_module[0].weight.dtype\n",
    "    elif hasattr(projector_module, 'weight'): # 단일 레이어 경우\n",
    "        proj_initial_dtype = projector_module.weight.dtype\n",
    "    else:\n",
    "        proj_initial_dtype = \"Unknown structure\"\n",
    "print(f\"Debug: Initial mm_projector.dtype (first layer weight): {proj_initial_dtype}\")\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # VisionTower.forward는 Tensor[B, C, H, W]를 받습니다.\n",
    "    # img_tensor가 list[Tensor]이면 하나의 배치로 합칩니다.\n",
    "    if isinstance(img_tensor, list):\n",
    "        # 만약 각 요소가 [C, H, W] 형태라면 stack, [1, C, H, W] 형태라면 cat\n",
    "        t0 = img_tensor[0]\n",
    "        if t0.dim() == 3:\n",
    "            images_tensor = torch.stack(img_tensor, dim=0)\n",
    "        else:\n",
    "            images_tensor = torch.cat(img_tensor, dim=0)\n",
    "    else:\n",
    "        images_tensor = img_tensor\n",
    "    # --- images_tensor dtype 확인 ---\n",
    "    print(f\"Debug: images_tensor shape: {images_tensor.shape}, dtype: {images_tensor.dtype}\")\n",
    "\n",
    "\n",
    "    # ——— 추가 디버깅 #1: 내부 vision_tower 모듈 확인 ———\n",
    "    vt = model.get_vision_tower()\n",
    "    if hasattr(vt, \"vision_tower\"):\n",
    "        print(\"Debug: inner vision_tower type:\", type(vt.vision_tower))\n",
    "        # --- 내부 vision_tower dtype 확인 ---\n",
    "        inner_vt_dtype = \"N/A\"\n",
    "        if hasattr(vt.vision_tower, 'dtype'):\n",
    "            inner_vt_dtype = vt.vision_tower.dtype\n",
    "        else:\n",
    "             try: inner_vt_dtype = next(vt.vision_tower.parameters()).dtype\n",
    "             except StopIteration: inner_vt_dtype = \"No Params\"\n",
    "        print(f\"Debug: inner vision_tower dtype: {inner_vt_dtype}\")\n",
    "        # ---------------------------------\n",
    "    else:\n",
    "        print(\"Debug: vision_tower has no .vision_tower attr\")\n",
    "\n",
    "    # ——— 추가 디버깅 #2: config 값 확인 ———\n",
    "    print(\"Debug: config.mm_vision_select_layer =\", getattr(model.config, \"mm_vision_select_layer\", None))\n",
    "    print(\"Debug: config.mm_projector_type      =\", getattr(model.config, \"mm_projector_type\", None))\n",
    "\n",
    "\n",
    "    # --- 직접 custom ViT 호출 (이전과 동일하나, 입력 dtype 확인) ---\n",
    "    vt_local = model.get_vision_tower()\n",
    "    # --- vt_local.vision_tower의 dtype 확인 ---\n",
    "    print(f\"Debug: dtype of vt_local.vision_tower before call: {vt_local.vision_tower.dtype if hasattr(vt_local.vision_tower, 'dtype') else 'N/A'}\")\n",
    "    # ---------------------------------------\n",
    "    print(\"Debug: Calling custom vision tower directly...\")\n",
    "    # --- 입력 images_tensor의 dtype을 vt_local.vision_tower의 dtype에 맞춰 전달 ---\n",
    "    # 이는 실제 CLIPVisionTower.forward 내부의 동작과 유사하게 만듦\n",
    "    try:\n",
    "         target_vt_dtype = next(vt_local.vision_tower.parameters()).dtype\n",
    "    except StopIteration: # 파라미터 없는 경우 대비\n",
    "         target_vt_dtype = vt_initial_dtype if vt_initial_dtype != \"N/A\" and vt_initial_dtype != \"No Params\" else torch.float32 # fallback\n",
    "    print(f\"Debug: Matching input tensor dtype to target_vt_dtype: {target_vt_dtype}\")\n",
    "    outputs_vt_direct = vt_local.vision_tower(images_tensor.to(target_vt_dtype), output_hidden_states=True) # hidden_states 얻기 위해 True\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    if not isinstance(outputs_vt_direct, BaseModelOutputWithPooling) or not hasattr(outputs_vt_direct, 'last_hidden_state'):\n",
    "         raise TypeError(f\"Expected BaseModelOutputWithPooling, but got {type(outputs_vt_direct)}\")\n",
    "\n",
    "    # --- last_hidden_state 추출 및 dtype 확인 ---\n",
    "    vt_hidden_states_direct = outputs_vt_direct.last_hidden_state\n",
    "    print(f\"Debug (direct vt): Extracted last_hidden_state shape: {tuple(vt_hidden_states_direct.shape)}, dtype: {vt_hidden_states_direct.dtype}\")\n",
    "\n",
    "    # --- 중간 레이어 특징 추출 (디버깅용, 실제 로직 모방) ---\n",
    "    hidden_states_tuple = outputs_vt_direct.hidden_states\n",
    "    selected_layer_features = None\n",
    "    selected_patch_features = None # 초기화 추가\n",
    "\n",
    "    # --- hidden_states_tuple 유효성 검사 ---\n",
    "    if hidden_states_tuple is not None and isinstance(hidden_states_tuple, tuple) and len(hidden_states_tuple) > 23: # 인덱스 23 접근 가능 확인\n",
    "        selected_layer_features = hidden_states_tuple[23] # -2 layer (index 23)\n",
    "        print(f\"Debug (direct vt): Extracted layer -2 features shape: {selected_layer_features.shape}, dtype: {selected_layer_features.dtype}\")\n",
    "        # 여기서 패치 슬라이싱 수행 (디버깅 목적)\n",
    "        num_registers = 4 # 예시 값 (실제로는 self.num_registers 사용해야 하나 여기서는 하드코딩)\n",
    "        num_patches = 256 # 예시 값 (실제로는 self.num_patches 사용해야 하나 여기서는 하드코딩)\n",
    "        start_index = 1 + num_registers\n",
    "        end_index = start_index + num_patches\n",
    "        if selected_layer_features.shape[1] >= end_index:\n",
    "             selected_patch_features = selected_layer_features[:, start_index:end_index]\n",
    "             print(f\"Debug (direct vt): Sliced layer -2 patch features shape: {selected_patch_features.shape}, dtype: {selected_patch_features.dtype}\")\n",
    "             # --- NaN/Inf 및 값 범위 확인 ---\n",
    "             print(f\"  Checking sliced patch feature values:\")\n",
    "             # --- .float() 캐스팅 추가하여 다양한 입력 dtype 처리 ---\n",
    "             selected_patch_features_float = selected_patch_features.float()\n",
    "             has_nan = torch.isnan(selected_patch_features_float).any()\n",
    "             has_inf = torch.isinf(selected_patch_features_float).any()\n",
    "             print(f\"    Has NaN: {has_nan}\")\n",
    "             print(f\"    Has Inf: {has_inf}\")\n",
    "             if not has_nan and not has_inf:\n",
    "                 print(f\"    Min: {selected_patch_features_float.min().item():.4f}\")\n",
    "                 print(f\"    Max: {selected_patch_features_float.max().item():.4f}\")\n",
    "                 print(f\"    Mean: {selected_patch_features_float.mean().item():.4f}\")\n",
    "                 print(f\"    Std: {selected_patch_features_float.std().item():.4f}\")\n",
    "             # -----------------------------\n",
    "        else:\n",
    "             print(\"Warning: Not enough tokens in selected layer feature to slice patches.\")\n",
    "             # selected_patch_features 는 None 유지\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract layer -2 features for detailed check. hidden_states_tuple is None or length {len(hidden_states_tuple) if hidden_states_tuple is not None else 'N/A'}\")\n",
    "        # selected_layer_features 는 None 유지\n",
    "\n",
    "    # --- !!! 프로젝터 테스트 시 float32 강제 !!! ---\n",
    "    try:\n",
    "        projector = model.get_model().mm_projector\n",
    "        # --- 프로젝터 유효성 확인 ---\n",
    "        if projector is None:\n",
    "            print(\"Error: mm_projector is None.\")\n",
    "        else:\n",
    "            # --- 프로젝터를 명시적으로 float32로 변환 ---\n",
    "            projector_f32 = projector.to(torch.float32)\n",
    "            # --- 변환 후 dtype 확인 ---\n",
    "            proj_f32_dtype = \"N/A\"\n",
    "            if isinstance(projector_f32, nn.Sequential) and len(projector_f32) > 0 and hasattr(projector_f32[0], 'weight'):\n",
    "                 proj_f32_dtype = projector_f32[0].weight.dtype\n",
    "            elif hasattr(projector_f32, 'weight'):\n",
    "                 proj_f32_dtype = projector_f32.weight.dtype\n",
    "            print(f\"Debug: Projector explicitly cast to dtype: {proj_f32_dtype}\")\n",
    "\n",
    "            # --- 사용할 입력 특징 선택 (계층적) ---\n",
    "            # 1순위: 슬라이싱된 패치 특징\n",
    "            # 2순위: 선택된 레이어 전체 특징\n",
    "            # 3순위: 최종 레이어 전체 특징 (last_hidden_state)\n",
    "            proj_input_tensor = None\n",
    "            input_source = \"None\"\n",
    "            if selected_patch_features is not None:\n",
    "                proj_input_tensor = selected_patch_features\n",
    "                input_source = \"Sliced layer -2 patches\"\n",
    "                print(f\"Debug: Using '{input_source}' for projector test.\")\n",
    "            elif selected_layer_features is not None:\n",
    "                proj_input_tensor = selected_layer_features\n",
    "                input_source = \"Full layer -2\"\n",
    "                print(f\"Debug: Using '{input_source}' for projector test (patch slice failed or not available).\")\n",
    "            elif vt_hidden_states_direct is not None:\n",
    "                 proj_input_tensor = vt_hidden_states_direct\n",
    "                 input_source = \"Last hidden state\"\n",
    "                 print(f\"Debug: Using '{input_source}' for projector test (layer -2 extraction failed or not available).\")\n",
    "            else:\n",
    "                 print(\"Error: No valid input tensor found for projector test.\")\n",
    "                 proj_input_tensor = None # 확실히 None 처리\n",
    "\n",
    "            if proj_input_tensor is not None:\n",
    "                # --- 입력 텐서도 명시적으로 float32로 변환 ---\n",
    "                proj_input_tensor_f32 = proj_input_tensor.to(torch.float32)\n",
    "                print(f\"Debug: Projector input tensor (from '{input_source}') explicitly cast to dtype: {proj_input_tensor_f32.dtype}\")\n",
    "\n",
    "                # 프로젝터 forward 실행 (float32 프로젝터와 float32 입력 사용)\n",
    "                proj_out_debug = projector_f32(proj_input_tensor_f32)\n",
    "                print(f\"Debug: Projector output dtype: {proj_out_debug.dtype}\") # 출력 dtype 확인\n",
    "                # --- 출력 값 확인 (NaN/Inf 포함) ---\n",
    "                # 출력도 .float()로 변환하여 계산 (이미 float32지만 명시적으로)\n",
    "                proj_out_debug_float = proj_out_debug.float()\n",
    "                print(\n",
    "                    f\"Debug: Projector output (float32 forced) | \"\n",
    "                    f\"Input source: '{input_source}', Input shape: {proj_input_tensor_f32.shape}, Output shape: {proj_out_debug_float.shape} | \"\n",
    "                    f\"sum={torch.nansum(proj_out_debug_float).item():.4f}, \" # NaN 무시하고 합계 계산\n",
    "                    f\"mean={torch.nanmean(proj_out_debug_float).item():.4f}, \" # NaN 무시하고 평균 계산\n",
    "                    f\"std={torch.nan_to_num(proj_out_debug_float, nan=0.0).std().item():.4f}\" # NaN을 0으로 바꿔 std 계산\n",
    "                )\n",
    "                has_inf_out = torch.isinf(proj_out_debug_float).any()\n",
    "                has_nan_out = torch.isnan(proj_out_debug_float).any() # NaN도 확인\n",
    "                print(f\"  Output has Inf: {has_inf_out}\")\n",
    "                print(f\"  Output has NaN: {has_nan_out}\") # NaN 결과 출력\n",
    "                if not has_inf_out and not has_nan_out:\n",
    "                     # 정상 범위 값 확인\n",
    "                     try:\n",
    "                          print(f\"  Output Min: {proj_out_debug_float.min().item():.4f}\")\n",
    "                          print(f\"  Output Max: {proj_out_debug_float.max().item():.4f}\")\n",
    "                     except RuntimeError as e_minmax: # min/max 연산 에러 처리 (예: 빈 텐서)\n",
    "                          print(f\"  Could not calculate Min/Max for output: {e_minmax}\")\n",
    "                # Inf 발생 시 추가 정보\n",
    "                if has_inf_out:\n",
    "                     inf_indices = torch.isinf(proj_out_debug_float).nonzero(as_tuple=True)\n",
    "                     print(f\"    Inf found at {len(inf_indices[0])} locations. First few indices: {[(inf_indices[0][i].item(), inf_indices[1][i].item(), inf_indices[2][i].item()) for i in range(min(5, len(inf_indices[0])))]}\")\n",
    "                     # Inf가 발생한 입력값 확인 시도 (매우 근사적)\n",
    "                     try:\n",
    "                          problematic_input_sample = proj_input_tensor_f32[inf_indices[0][0], inf_indices[1][0]].detach().cpu().numpy()\n",
    "                          print(f\"    Sample input vector near first Inf location (approx): mean={problematic_input_sample.mean():.4f}, std={problematic_input_sample.std():.4f}, min={problematic_input_sample.min():.4f}, max={problematic_input_sample.max():.4f}\")\n",
    "                     except Exception as e_inf_input:\n",
    "                          print(f\"    Could not get input sample near Inf: {e_inf_input}\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"Debug: Skipping projector debug check due to missing input tensor.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during projector debug check: {e}\")\n",
    "\n",
    "\n",
    "# ==== 디버깅 코드 추가 종료 ====\n",
    "\n",
    "# --- dtype mismatch 방지: 모델 전체를 float32로 변환 (선택적이지만 권장) ---\n",
    "# 모델 로드 시 torch_dtype=torch.float32 를 사용했으므로 이 부분은 주석 처리 가능\n",
    "print(f\"\\nCasting the entire model to float32...\")\n",
    "model = model.to(torch.float32)\n",
    "# 다시 한번 dtype 확인\n",
    "print(f\"\\nVerifying final model dtypes...\")\n",
    "print(f\"Debug: Final model.dtype: {model.dtype}\")\n",
    "vt_final_dtype = \"N/A\"\n",
    "if model.get_vision_tower() is not None and hasattr(model.get_vision_tower(), 'dtype'): vt_final_dtype = model.get_vision_tower().dtype\n",
    "elif model.get_vision_tower() is not None:\n",
    "     try: vt_final_dtype = next(model.get_vision_tower().parameters()).dtype\n",
    "     except StopIteration: vt_final_dtype = \"No Params\"\n",
    "print(f\"Debug: Final Vision_tower dtype: {vt_final_dtype}\")\n",
    "proj_final_dtype = \"N/A\"\n",
    "if hasattr(model.get_model(), 'mm_projector') and model.get_model().mm_projector is not None:\n",
    "    projector_module = model.get_model().mm_projector\n",
    "    if isinstance(projector_module, nn.Sequential) and len(projector_module) > 0 and hasattr(projector_module[0], 'weight'): proj_final_dtype = projector_module[0].weight.dtype\n",
    "    elif hasattr(projector_module, 'weight'): proj_final_dtype = projector_module.weight.dtype\n",
    "    else: proj_final_dtype = \"Unknown structure\"\n",
    "print(f\"Debug: Final mm_projector dtype (first layer weight): {proj_final_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806fa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 5. 프롬프트 구성\n",
    "# ---------------------------------------------------------------------------\n",
    "if model.config.mm_use_im_start_end:\n",
    "    prompt_user = f\"{DEFAULT_IM_START_TOKEN}{DEFAULT_IMAGE_TOKEN}{DEFAULT_IM_END_TOKEN}\\n{USER_PROMPT}\"\n",
    "else:\n",
    "    prompt_user = f\"{DEFAULT_IMAGE_TOKEN}\\n{USER_PROMPT}\"\n",
    "\n",
    "conv.append_message(roles[0], prompt_user)\n",
    "conv.append_message(roles[1], None)\n",
    "full_prompt = conv.get_prompt()\n",
    "\n",
    "print(\"--- Prompt to tokenizer ---\")\n",
    "print(full_prompt)\n",
    "\n",
    "input_ids = tokenizer_image_token(\n",
    "    full_prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n",
    ").unsqueeze(0).to(model.device)\n",
    "\n",
    "# --- 디버그 가드: <image> 토큰이 꼭 포함됐는지 확인 ---\n",
    "assert (input_ids == IMAGE_TOKEN_INDEX).sum().item() > 0, \\\n",
    "       \"IMAGE_TOKEN_INDEX (-200) 가 input_ids 에 없습니다. <image> 토큰 치환 실패!\"\n",
    "\n",
    "attention_mask = torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 6. (옵션) Cross‑Attention Heatmaps\n",
    "# ---------------------------------------------------------------------------\n",
    "VISUALIZE = True\n",
    "if VISUALIZE:\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 6‑A.  Patch → Patch  (vision‑tower 직후) heat‑map\n",
    "    # ----------------------------------------------------------------------\n",
    "    with torch.no_grad():\n",
    "        outs_vt = model(\n",
    "            input_ids=input_ids,\n",
    "            images=img_tensor,\n",
    "            image_sizes=[img_size],\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "    attn_vt = outs_vt.attentions[-1].sum(dim=1)[0].cpu()       # (seq, seq)\n",
    "\n",
    "    # 이미지 패치 구간 인덱스 계산 (generate 전에 미리 저장)\n",
    "    img_tok_pos = (input_ids[0] == IMAGE_TOKEN_INDEX).nonzero(as_tuple=True)[0].item()\n",
    "    patch_len   = attn_vt.size(1) - img_tok_pos                # 전체 패치 수\n",
    "    patch_vec   = attn_vt[img_tok_pos + 1,                     # Q: 첫 패치\n",
    "                          img_tok_pos : img_tok_pos + patch_len]\n",
    "\n",
    "    L = patch_vec.shape[0]\n",
    "    grid = math.ceil(math.sqrt(L))\n",
    "    if grid * grid != L:\n",
    "        patch_vec = F.interpolate(\n",
    "            patch_vec.float()[None,None],\n",
    "            size=grid * grid,\n",
    "            mode=\"linear\",\n",
    "            align_corners=False\n",
    "        )[0,0].to(patch_vec.dtype)\n",
    "    patch_map = patch_vec.float().reshape(grid, grid)\n",
    "    heatA = F.interpolate(\n",
    "        patch_map[None,None],\n",
    "        size=pil_img.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )[0,0].cpu().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title(\"Patch → Patch attention\")\n",
    "    plt.imshow(overlay_heatmap(heatA, pil_img)); plt.axis(\"off\"); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ---------------------------------------------------------------------------\n",
    "# 7. 텍스트 생성\n",
    "# ---------------------------------------------------------------------------\n",
    "print(f\"\\n[{roles[0]}] {USER_PROMPT}\")\n",
    "print(f\"[{roles[1]}] \", flush=True)\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "gen_kwargs = dict(\n",
    "    inputs=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    images=img_tensor,\n",
    "    image_sizes=[img_size],\n",
    "    do_sample=TEMPERATURE > 0,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
    "    output_attentions=True,\n",
    "    return_dict_in_generate=True\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_gen = model.generate(**gen_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cdbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 8. (옵션) 프롬프트 + 생성 토큰까지 포함한 Text → Patch heat‑map\n",
    "# ---------------------------------------------------------------------------\n",
    "if VISUALIZE:\n",
    "    # out_gen.sequences 는 <image> 토큰이 빠져 있으므로,\n",
    "    # 원본 input_ids 와 생성 토큰(gen_only)을 이어 붙여 완전한 시퀀스 복원\n",
    "    gen_only   = out_gen.sequences[:, input_ids.size(1):]\n",
    "    seqs_full  = torch.cat([input_ids, gen_only], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs_full = model(\n",
    "            input_ids=seqs_full,\n",
    "            images=img_tensor,\n",
    "            image_sizes=[img_size],\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    full_attn = outs_full.attentions[-1].sum(dim=1)[0].cpu()   # (seq, seq)\n",
    "\n",
    "    # 프롬프트 / 생성 토큰 (텍스트 영역) ➜ 이미지 패치 구간\n",
    "    text_start = img_tok_pos + patch_len\n",
    "    text_end   = full_attn.size(1)\n",
    "    txt2img_vec = full_attn[text_start:text_end,\n",
    "                            img_tok_pos : img_tok_pos + patch_len].mean(dim=0)\n",
    "    # ── 수정: NaN 방지 ───────────────────────────────────────────\n",
    "    txt2img_vec = torch.nan_to_num(txt2img_vec, nan=0.0)\n",
    "    # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "    L   = txt2img_vec.shape[0]\n",
    "    grid = math.ceil(math.sqrt(L))\n",
    "    if grid * grid != L:\n",
    "        txt2img_vec = F.interpolate(\n",
    "            txt2img_vec.float()[None,None],\n",
    "            size=grid * grid,\n",
    "            mode=\"linear\",\n",
    "            align_corners=False\n",
    "        )[0,0].to(txt2img_vec.dtype)\n",
    "    txt_map = txt2img_vec.float().reshape(grid, grid)\n",
    "    heatB = F.interpolate(\n",
    "        txt_map[None,None],\n",
    "        size=pil_img.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    )[0,0].cpu().numpy()\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.title(\"Text → Patch attention (prompt + generation)\")\n",
    "    plt.imshow(overlay_heatmap(heatB, pil_img)); plt.axis(\"off\"); plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# **Inference & Visualization Complete.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
