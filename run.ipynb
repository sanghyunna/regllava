{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7572b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for user: sudo: ap: command not found\n",
      "[sudo] password for user: \n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "build-essential is already the newest version (12.10ubuntu1).\n",
      "g++ is already the newest version (4:13.2.0-7ubuntu1).\n",
      "gcc is already the newest version (4:13.2.0-7ubuntu1).\n",
      "make is already the newest version (4.3-4.1build2).\n",
      "ninja-build is already the newest version (1.11.1-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 247 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "password = getpass.getpass()\n",
    "command = \"sudo -S ap update\" #can be any command but don't forget -S as it enables input from stdin\n",
    "os.system('echo %s | %s' % (password, command))\n",
    "command = \"sudo -S apt install -y build-essential g++ gcc make ninja-build\" #can be any command but don't forget -S as it enables input from stdin\n",
    "os.system('echo %s | %s' % (password, command))\n",
    "\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-12.4\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "    \"/usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.4/lib64:\"\n",
    "    + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308c24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_reg_gated_vit.py\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPVisionConfig, CLIPVisionModel\n",
    "from INFERclipregXGATED.model import VisionTransformer\n",
    "\n",
    "# 1) 커스텀 비전 트랜스포머 인스턴스 생성 & 체크포인트 로드\n",
    "VISION_ENCODER_CONFIG = dict(\n",
    "    image_resolution=224, patch_size=14,\n",
    "    width=1024, layers=24, heads=16,\n",
    "    output_dim=1024, num_registers=4\n",
    ")\n",
    "custom_vt = VisionTransformer(\n",
    "    input_resolution=VISION_ENCODER_CONFIG['image_resolution'],\n",
    "    patch_size=      VISION_ENCODER_CONFIG['patch_size'],\n",
    "    width=           VISION_ENCODER_CONFIG['width'],\n",
    "    layers=          VISION_ENCODER_CONFIG['layers'],\n",
    "    heads=           VISION_ENCODER_CONFIG['heads'],\n",
    "    output_dim=      VISION_ENCODER_CONFIG['output_dim'],\n",
    "    num_registers=   VISION_ENCODER_CONFIG['num_registers'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdba304b",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '`'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# (예: safetensors 사용 시)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 여기서는 state_dict 키에서 \"visual.\" prefix 제거\u001b[39;00m\n\u001b[32m      4\u001b[39m vt_state = {k.split(\u001b[33m\"\u001b[39m\u001b[33mvisual.\u001b[39m\u001b[33m\"\u001b[39m,\u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m]:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m state.items() \u001b[38;5;28;01mif\u001b[39;00m k.startswith(\u001b[33m\"\u001b[39m\u001b[33mvisual.\u001b[39m\u001b[33m\"\u001b[39m)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava_fresh/lib/python3.11/site-packages/torch/serialization.py:1040\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1039\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(UNSAFE_MESSAGE + \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/regllava_fresh/lib/python3.11/site-packages/torch/serialization.py:1258\u001b[39m, in \u001b[36m_legacy_load\u001b[39m\u001b[34m(f, map_location, pickle_module, **pickle_load_args)\u001b[39m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[33m'\u001b[39m\u001b[33mreadinto\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m0\u001b[39m) <= sys.version_info < (\u001b[32m3\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1254\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1255\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[33m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfunctionality.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1258\u001b[39m magic_number = \u001b[43mpickle_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic_number != MAGIC_NUMBER:\n\u001b[32m   1260\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid magic number; corrupt file?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mUnpicklingError\u001b[39m: invalid load key, '`'."
     ]
    }
   ],
   "source": [
    "# (예: safetensors 사용 시)\n",
    "state = torch.load(\"models/ViT-L-14-REG-GATED-balanced-ckpt12.safetensors\", map_location=\"cpu\")\n",
    "# 여기서는 state_dict 키에서 \"visual.\" prefix 제거\n",
    "vt_state = {k.split(\"visual.\",1)[-1]:v for k,v in state.items() if k.startswith(\"visual.\")}\n",
    "custom_vt.load_state_dict(vt_state, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4946d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) HF CLIPVisionModel 구성 같은 아키텍처로 생성\n",
    "hf_cfg = CLIPVisionConfig(\n",
    "    image_size=224,\n",
    "    patch_size=14,\n",
    "    hidden_size=1024,\n",
    "    num_hidden_layers=24,\n",
    "    num_attention_heads=16,\n",
    "    intermediate_size=1024 * 4,   # typically 4× hidden_size\n",
    ")\n",
    "hf_model = CLIPVisionModel(hf_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388f7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) 커스텀 가중치를 HF 모델에 복사\n",
    "#    -- custom_vt.state_dict()의 키와 hf_model.visual.state_dict()의 키가 일치해야 합니다.\n",
    "hf_state = hf_model.visual.state_dict()\n",
    "for name, tensor in custom_vt.state_dict().items():\n",
    "    if name in hf_state and hf_state[name].shape == tensor.shape:\n",
    "        hf_state[name] = tensor\n",
    "hf_model.visual.load_state_dict(hf_state, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70640eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) HF 구조로 저장\n",
    "hf_model.save_pretrained(\"./reg_gated_vit_l14\")\n",
    "print(\"✅ Saved Reg-Gated ViT-L/14 as HF format in ./reg_gated_vit_l14\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regllava_fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
